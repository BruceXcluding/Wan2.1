# FastAPI Multi-GPU Video Generation API

åŸºäº Wan2.1-I2V-14B-720P æ¨¡å‹çš„å¤šå¡åˆ†å¸ƒå¼è§†é¢‘ç”Ÿæˆ API æœåŠ¡ï¼Œæ”¯æŒå›¾åƒåˆ°è§†é¢‘ï¼ˆImage-to-Videoï¼‰ç”Ÿæˆã€‚é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„è®¾è®¡ï¼Œæ”¯æŒåä¸ºæ˜‡è…¾ NPU å’Œ NVIDIA GPU å¤šå¡åˆ†å¸ƒå¼æ¨ç†ã€‚

## ğŸš€ é¡¹ç›®ç‰¹è‰²

- **ğŸ¯ å¤šå¡åˆ†å¸ƒå¼**ï¼šæ”¯æŒ NPU/GPU 8å¡å¹¶è¡Œæ¨ç†ï¼Œè‡ªåŠ¨è®¾å¤‡æ£€æµ‹
- **ğŸ§  T5 CPU æ¨¡å¼**ï¼šæ”¯æŒ T5 æ–‡æœ¬ç¼–ç å™¨åœ¨ CPU ä¸Šè¿è¡Œï¼ŒèŠ‚çœæ˜¾å­˜
- **ğŸ”„ å¼‚æ­¥å¤„ç†**ï¼šåŸºäº FastAPI çš„å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—å’ŒçŠ¶æ€ç®¡ç†
- **ğŸ§© æ¨¡å—åŒ–æ¶æ„**ï¼šæ¸…æ™°çš„åˆ†å±‚è®¾è®¡ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•
- **âš¡ æ€§èƒ½ä¼˜åŒ–**ï¼šæ³¨æ„åŠ›ç¼“å­˜ã€VAEå¹¶è¡Œç­‰å¤šç§åŠ é€ŸæŠ€æœ¯
- **ğŸ“Š ä»»åŠ¡ç®¡ç†**ï¼šå®Œæ•´çš„ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†å’Œé˜Ÿåˆ—æ§åˆ¶
- **ğŸ›¡ï¸ å®¹é”™æœºåˆ¶**ï¼šå¥å£®çš„é”™è¯¯å¤„ç†å’Œèµ„æºæ¸…ç†
- **ğŸ”’ ä¼ä¸šçº§å®‰å…¨**ï¼šèµ„æºé™åˆ¶ã€å¹¶å‘æ§åˆ¶ã€å¼‚å¸¸å¤„ç†
- **ğŸ“ˆ ç›‘æ§è¿ç»´**ï¼šå¥åº·æ£€æŸ¥ã€æ€§èƒ½ç›‘æ§ã€è°ƒè¯•å·¥å…·
- **ğŸ›ï¸ æ™ºèƒ½é…ç½®**ï¼šé…ç½®ç”Ÿæˆå™¨ã€ç¯å¢ƒé¢„è®¾ã€å‚æ•°éªŒè¯

## ğŸ“ é¡¹ç›®ç»“æ„

```
fastapi-multigpu-i2v/
â”œâ”€â”€ src/                              # ğŸ¯ æ ¸å¿ƒæºç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ i2v_api.py                    # FastAPI ä¸»åº”ç”¨
â”‚   â”œâ”€â”€ schemas/                      # ğŸ“‹ æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ video.py                  # è¯·æ±‚/å“åº”æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ services/                     # ğŸ”§ ä¸šåŠ¡é€»è¾‘å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ video_service.py          # ä»»åŠ¡ç®¡ç†æœåŠ¡ (å¾…å®ç°)
â”‚   â”œâ”€â”€ pipelines/                    # ğŸš€ æ¨ç†ç®¡é“
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_pipeline.py          # ç®¡é“åŸºç±» (ä½¿ç”¨æ··å…¥ç±»é‡æ„)
â”‚   â”‚   â”œâ”€â”€ npu_pipeline.py           # NPU ç®¡é“å®ç° (ç®€åŒ–ç‰ˆ)
â”‚   â”‚   â”œâ”€â”€ cuda_pipeline.py          # CUDA ç®¡é“å®ç° (ç®€åŒ–ç‰ˆ)
â”‚   â”‚   â””â”€â”€ pipeline_factory.py       # ç®¡é“å·¥å‚
â”‚   â””â”€â”€ utils/                        # ğŸ› ï¸ å†…éƒ¨å·¥å…·ç±»
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ utils/                            # ğŸ› ï¸ é¡¹ç›®çº§å·¥å…·
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ device_detector.py            # è®¾å¤‡è‡ªåŠ¨æ£€æµ‹
â”œâ”€â”€ scripts/                          # ğŸ“œ å¯åŠ¨è„šæœ¬
â”‚   â”œâ”€â”€ start_service_npu.sh          # NPU ä¸“ç”¨å¯åŠ¨è„šæœ¬ (ä¼˜åŒ–ç‰ˆ)
â”‚   â”œâ”€â”€ start_service_cuda.sh         # CUDA ä¸“ç”¨å¯åŠ¨è„šæœ¬ (æ–°å¢)
â”‚   â”œâ”€â”€ start_service_general.sh      # é€šç”¨æ™ºèƒ½å¯åŠ¨è„šæœ¬ (ä¼˜åŒ–ç‰ˆ)
â”‚   â””â”€â”€ debug/                        # ğŸ” è°ƒè¯•å·¥å…·é›†
â”‚       â”œâ”€â”€ README.md                 # è°ƒè¯•å·¥å…·ä½¿ç”¨æŒ‡å—
â”‚       â”œâ”€â”€ debug_device.py           # è®¾å¤‡æ£€æµ‹è°ƒè¯• (å®Œæ•´ç‰ˆ)
â”‚       â”œâ”€â”€ debug_pipeline.py         # ç®¡é“è°ƒè¯• (å®Œæ•´ç‰ˆ)
â”‚       â”œâ”€â”€ debug_memory.py           # å†…å­˜ç›‘æ§è°ƒè¯•
â”‚       â””â”€â”€ debug_t5_warmup.py        # T5 é¢„çƒ­è°ƒè¯•
â”œâ”€â”€ tools/                            # ğŸ› ï¸ å¼€å‘å·¥å…·é›†
â”‚   â”œâ”€â”€ README.md                     # å¼€å‘å·¥å…·ä½¿ç”¨æŒ‡å—
â”‚   â”œâ”€â”€ verify_structure.py           # é¡¹ç›®ç»“æ„éªŒè¯ (å®Œæ•´ç‰ˆ)
â”‚   â”œâ”€â”€ config_generator.py           # æ™ºèƒ½é…ç½®ç”Ÿæˆå™¨ (å®Œæ•´ç‰ˆ)
â”‚   â”œâ”€â”€ benchmark.py                  # æ€§èƒ½åŸºå‡†æµ‹è¯•
â”‚   â””â”€â”€ health_monitor.py             # å¥åº·ç›‘æ§å·¥å…·
â”œâ”€â”€ tests/                            # âœ… æµ‹è¯•ç”¨ä¾‹
â”œâ”€â”€ docs/                             # ğŸ“š é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ generated_videos/                 # ğŸ“¹ ç”Ÿæˆè§†é¢‘å­˜å‚¨
â”œâ”€â”€ logs/                             # ğŸ“ æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ requirements.txt                  # ä¾èµ–æ¸…å•
â””â”€â”€ README.md                         # é¡¹ç›®æ–‡æ¡£
```

## ğŸ”§ ç¯å¢ƒè¦æ±‚

### ç¡¬ä»¶æ”¯æŒ

#### NPU (åä¸ºæ˜‡è…¾)
- **è®¾å¤‡å‹å·**ï¼š910B1/910B2/910B4 ç­‰æ˜‡è…¾èŠ¯ç‰‡
- **æ˜¾å­˜è¦æ±‚**ï¼šå•å¡ 24GB+ (T5 CPU æ¨¡å¼) / 32GB+ (æ ‡å‡†æ¨¡å¼)
- **é©±åŠ¨ç‰ˆæœ¬**ï¼šCANN 8.0+

#### GPU (NVIDIA)
- **è®¾å¤‡å‹å·**ï¼šRTX 3090/4090, A100, H100 ç­‰
- **æ˜¾å­˜è¦æ±‚**ï¼šå•å¡ 24GB+ (æ¨è 32GB+)
- **é©±åŠ¨ç‰ˆæœ¬**ï¼šCUDA 11.8+ / CUDA 12.0+

### ç³»ç»Ÿè¦æ±‚
- **CPU**ï¼š16+ æ ¸å¿ƒ (T5 CPU æ¨¡å¼å»ºè®® 32+ æ ¸å¿ƒ)
- **å†…å­˜**ï¼š64GB+ ç³»ç»Ÿå†…å­˜ (T5 CPU æ¨¡å¼éœ€è¦æ›´å¤š)
- **å­˜å‚¨**ï¼š200GB+ å¯ç”¨ç©ºé—´ (æ¨¡å‹ + è¾“å‡ºè§†é¢‘)
- **æ“ä½œç³»ç»Ÿ**ï¼šLinux (æ¨è Ubuntu 20.04+)

### è½¯ä»¶ç¯å¢ƒ
- **Python**ï¼š3.10+
- **PyTorch**ï¼š2.0+
- **è®¾å¤‡æ‰©å±•**ï¼štorch_npu (NPU) / torch (CUDA)

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### 1. é¡¹ç›®åˆå§‹åŒ–ä¸éªŒè¯

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd fastapi-multigpu-i2v

# ğŸ” å®Œæ•´é¡¹ç›®éªŒè¯ (æ¨èé¦–æ¬¡è¿è¡Œ)
python3 tools/verify_structure.py

# ğŸ¯ å¿«é€Ÿè®¾å¤‡æ£€æµ‹
python3 scripts/debug/debug_device.py

# ğŸ“¦ æ£€æŸ¥å¯ç”¨ç®¡é“
python3 -c "from pipelines import get_available_pipelines; print(f'Available: {get_available_pipelines()}')"
```

### 2. æ™ºèƒ½é…ç½®ç”Ÿæˆ

```bash
# ğŸ¤– è‡ªåŠ¨ç”Ÿæˆæœ€ä¼˜é…ç½® (æ ¹æ®ç¡¬ä»¶ç¯å¢ƒ)
python3 tools/config_generator.py --template production --export-env --output-dir configs/

# ğŸ”§ ä¸ºä¸åŒç¯å¢ƒç”Ÿæˆé…ç½®
python3 tools/config_generator.py --template development --export-env --output-dir configs/
python3 tools/config_generator.py --template testing --export-env --output-dir configs/

# ğŸ›ï¸ è‡ªå®šä¹‰é…ç½®ç”Ÿæˆ
python3 tools/config_generator.py \
  --template production \
  --custom '{"max_concurrent_tasks": 2, "t5_cpu": true}' \
  --model-path /your/model/path \
  --port 8080 \
  --export-env

# ğŸ“‹ æŸ¥çœ‹ç”Ÿæˆçš„é…ç½®
cat configs/config_production.env
```

### 3. ç¯å¢ƒé…ç½®

#### ä½¿ç”¨ç”Ÿæˆçš„é…ç½®æ–‡ä»¶
```bash
# åŠ è½½ç”Ÿäº§é…ç½®
source configs/config_production.env

# æˆ–è€…æ‰‹åŠ¨è®¾ç½®å…³é”®å˜é‡
export MODEL_CKPT_DIR="/data/models/modelscope/hub/Wan-AI/Wan2.1-I2V-14B-720P"
export T5_CPU=true
export MAX_CONCURRENT_TASKS=2
```

#### NPU ä¸“ç”¨ç¯å¢ƒå˜é‡
```bash
export ASCEND_LAUNCH_BLOCKING=0
export HCCL_TIMEOUT=2400
export HCCL_BUFFSIZE=512
export PYTORCH_NPU_ALLOC_CONF="expandable_segments:True"
```

#### CUDA ä¸“ç”¨ç¯å¢ƒå˜é‡
```bash
export NCCL_TIMEOUT=1800
export CUDA_LAUNCH_BLOCKING=0
```

### 4. ä¾èµ–å®‰è£…

```bash
# åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# éªŒè¯è®¾å¤‡ç¯å¢ƒ
python3 scripts/debug/debug_device.py
```

### 5. é¢„å¯åŠ¨è°ƒè¯• (æ¨è)

```bash
# ğŸ§ª T5 é¢„çƒ­æµ‹è¯• (T5 CPU æ¨¡å¼é‡è¦)
python3 scripts/debug/debug_t5_warmup.py --warmup-steps 3

# ğŸ§  å†…å­˜çŠ¶æ€æ£€æŸ¥
python3 scripts/debug/debug_memory.py --mode status

# ğŸ”— ç®¡é“åˆ›å»ºæµ‹è¯•
python3 scripts/debug/debug_pipeline.py --mode quick

# ğŸ“Š ç³»ç»Ÿèµ„æºæ£€æŸ¥
python3 tools/health_monitor.py --mode check
```

### 6. å¯åŠ¨æœåŠ¡

#### ğŸš€ æ™ºèƒ½å¯åŠ¨ (æ¨è)
```bash
# è‡ªåŠ¨æ£€æµ‹è®¾å¤‡å¹¶å¯åŠ¨æœ€ä¼˜é…ç½®
chmod +x scripts/start_service_general.sh
./scripts/start_service_general.sh
```

#### ğŸ¯ è®¾å¤‡ä¸“ç”¨å¯åŠ¨
```bash
# NPU ä¸“ç”¨å¯åŠ¨ (åä¸ºæ˜‡è…¾)
chmod +x scripts/start_service_npu.sh
./scripts/start_service_npu.sh

# GPU ä¸“ç”¨å¯åŠ¨ (NVIDIA CUDA)  
chmod +x scripts/start_service_cuda.sh
./scripts/start_service_cuda.sh
```

#### ğŸ›ï¸ è‡ªå®šä¹‰é…ç½®å¯åŠ¨
```bash
# T5 CPU æ¨¡å¼ (èŠ‚çœæ˜¾å­˜)
T5_CPU=true MAX_CONCURRENT_TASKS=2 ./scripts/start_service_npu.sh

# é«˜æ€§èƒ½æ¨¡å¼ (éœ€è¦å¤§æ˜¾å­˜)
T5_CPU=false MAX_CONCURRENT_TASKS=4 ./scripts/start_service_cuda.sh

# è°ƒè¯•æ¨¡å¼ (å•ä»»åŠ¡,é•¿è¶…æ—¶)
MAX_CONCURRENT_TASKS=1 TASK_TIMEOUT=3600 ./scripts/start_service_general.sh
```

### 7. æœåŠ¡éªŒè¯ä¸ç›‘æ§

```bash
# ğŸ¥ å¥åº·æ£€æŸ¥
curl http://localhost:8088/health

# ğŸ“Š è®¾å¤‡ä¿¡æ¯
curl http://localhost:8088/device-info

# ğŸ“ˆ ç›‘æ§æŒ‡æ ‡
curl http://localhost:8088/metrics

# ğŸ“– API æ–‡æ¡£
open http://localhost:8088/docs

# ğŸ” æŒç»­å¥åº·ç›‘æ§ (å¯é€‰)
python3 tools/health_monitor.py --mode monitor --duration 3600 &
```

## ğŸ” è°ƒè¯•å·¥å…·è¯¦è§£

### è®¾å¤‡æ£€æµ‹è°ƒè¯•
```bash
# ğŸ¯ å®Œæ•´è®¾å¤‡æ£€æµ‹ (åŒ…å«å†…å­˜æµ‹è¯•å’Œå…¼å®¹æ€§æ£€æŸ¥)
python3 scripts/debug/debug_device.py

# è¾“å‡ºç¤ºä¾‹ï¼š
# âœ… Detected device: npu
# âœ… Device count: 8
# âœ… Backend: torch_npu
# ğŸ“Š Device Details...
# ğŸ§  Memory Operations Test...
# ğŸ”— Pipeline Compatibility...
```

### T5 é¢„çƒ­è°ƒè¯• (é‡è¦!)
```bash
# ğŸ§  T5 CPU æ¨¡å¼é¢„çƒ­æµ‹è¯• (é¦–æ¬¡å¯åŠ¨å‰æ¨è)
python3 scripts/debug/debug_t5_warmup.py --warmup-steps 3

# ğŸ›ï¸ è‡ªå®šä¹‰æµ‹è¯•å‚æ•°
python3 scripts/debug/debug_t5_warmup.py \
  --model-path /path/to/model \
  --warmup-steps 5 \
  --skip-resource-check

# è¾“å‡ºç¤ºä¾‹ï¼š
# ğŸ” T5 Warmup Debug Tool
# âœ… T5 warmup completed successfully
# ğŸ“Š Warmup time: 45.2s
# ğŸ§  Memory usage: 12.5GB
```

### å†…å­˜ç›‘æ§è°ƒè¯•
```bash
# ğŸ“Š æŸ¥çœ‹å½“å‰å†…å­˜çŠ¶æ€
python3 scripts/debug/debug_memory.py --mode status

# ğŸ“ˆ è¿ç»­ç›‘æ§ 60 ç§’
python3 scripts/debug/debug_memory.py --mode monitor --duration 60 --interval 5

# ğŸ§ª æ¨¡å‹åŠ è½½å†…å­˜æµ‹è¯•
python3 scripts/debug/debug_memory.py --mode model-test

# ğŸ’¥ å†…å­˜å‹åŠ›æµ‹è¯•
python3 scripts/debug/debug_memory.py --mode stress-test

# ğŸ“‹ å¯¼å‡ºç›‘æ§æ•°æ®
python3 scripts/debug/debug_memory.py \
  --mode monitor \
  --duration 300 \
  --export memory_monitor.csv
```

### ç®¡é“ç³»ç»Ÿè°ƒè¯•
```bash
# ğŸ”§ å¿«é€Ÿç®¡é“æµ‹è¯•
python3 scripts/debug/debug_pipeline.py --mode quick

# ğŸ§ª ç»¼åˆç®¡é“æµ‹è¯• (åŒ…å«æ¨¡å‹åŠ è½½)
python3 scripts/debug/debug_pipeline.py --mode comprehensive

# ğŸ›ï¸ è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„æµ‹è¯•
python3 scripts/debug/debug_pipeline.py --model-path /path/to/model

# è¾“å‡ºç¤ºä¾‹ï¼š
# ğŸ”§ Pipeline Creation Test
# âœ… Pipeline created successfully in 25.3s
# âœ… Pipeline type: NPUPipeline
# âœ… Memory logging works
```

### è°ƒè¯•å·¥å…·æ‰¹é‡è¿è¡Œ
```bash
# ğŸ“‹ è¿è¡Œæ‰€æœ‰æ ¸å¿ƒè°ƒè¯•æ£€æŸ¥
echo "ğŸ” Running comprehensive debug checks..."
python3 scripts/debug/debug_device.py && \
python3 scripts/debug/debug_memory.py --mode status && \
python3 scripts/debug/debug_pipeline.py --mode quick && \
echo "âœ… All debug checks completed!"
```

## ğŸ› ï¸ å¼€å‘å·¥å…·è¯¦è§£

### é¡¹ç›®ç»“æ„éªŒè¯
```bash
# ğŸ” å®Œæ•´é¡¹ç›®éªŒè¯ (éƒ¨ç½²å‰å¿…æ£€)
python3 tools/verify_structure.py

# æ£€æŸ¥å†…å®¹ï¼š
# - ğŸ“„ å¿…éœ€æ–‡ä»¶å’Œç›®å½•
# - ğŸ Pythonå¯¼å…¥
# - ğŸ“¦ ä¾èµ–åŒ…
# - âš™ï¸ é…ç½®æ–‡ä»¶
# - ğŸŒ è¿è¡Œæ—¶ç¯å¢ƒ
# - ğŸ” æ–‡ä»¶æƒé™

# è¾“å‡ºç¤ºä¾‹ï¼š
# ğŸ‰ PROJECT VERIFICATION PASSED!
# âœ… Your project structure is ready for deployment
```

### æ™ºèƒ½é…ç½®ç”Ÿæˆå™¨
```bash
# ğŸ¤– è‡ªåŠ¨ç¡¬ä»¶æ£€æµ‹é…ç½®ç”Ÿæˆ
python3 tools/config_generator.py --template production

# ğŸ¯ ç”Ÿæˆä¸åŒç¯å¢ƒé…ç½®
python3 tools/config_generator.py --template development --export-env
python3 tools/config_generator.py --template testing --export-env
python3 tools/config_generator.py --template memory_efficient --export-env

# ğŸ›ï¸ é«˜çº§è‡ªå®šä¹‰é…ç½®
python3 tools/config_generator.py \
  --template production \
  --custom '{"t5_cpu": false, "max_concurrent_tasks": 6, "use_attentioncache": true}' \
  --model-path /high/performance/model \
  --port 8080 \
  --export-env \
  --export-json

# ğŸ“‹ é…ç½®å¯¹æ¯”
echo "Development vs Production:"
diff configs/config_development.env configs/config_production.env
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•
```bash
# âš¡ åŸºç¡€æ€§èƒ½æµ‹è¯•
python3 tools/benchmark.py --response-test 10

# ğŸš€ è´Ÿè½½æµ‹è¯•
python3 tools/benchmark.py --load-test --concurrent-users 5 --duration 300

# ğŸ“Š å®Œæ•´åŸºå‡†æµ‹è¯•å¥—ä»¶
python3 tools/benchmark.py \
  --response-test 20 \
  --load-test \
  --concurrent-users 10 \
  --duration 600 \
  --export benchmark_results.json

# ğŸ“ˆ æ€§èƒ½ç›‘æ§ (ä¸åŸºå‡†æµ‹è¯•é…åˆ)
python3 tools/health_monitor.py --mode monitor --duration 600 &
python3 tools/benchmark.py --load-test --concurrent-users 8
```

### å¥åº·ç›‘æ§å·¥å…·
```bash
# ğŸ¥ å•æ¬¡å¥åº·æ£€æŸ¥
python3 tools/health_monitor.py --mode check

# ğŸ“Š æŒç»­ç›‘æ§ 30 åˆ†é’Ÿ
python3 tools/health_monitor.py --mode monitor --duration 1800 --interval 30

# ğŸ“‹ å¯¼å‡ºç›‘æ§æ•°æ® (CSV/JSON)
python3 tools/health_monitor.py \
  --mode monitor \
  --duration 3600 \
  --export health_data.json \
  --alert-on-error \
  --alert-on-high-memory

# ğŸ”” å‘Šè­¦ç›‘æ§æ¨¡å¼
python3 tools/health_monitor.py \
  --mode monitor \
  --duration 86400 \
  --alert-on-error \
  --alert-threshold-memory 90 \
  --alert-threshold-response-time 30
```

### å·¥å…·ç»„åˆä½¿ç”¨ç¤ºä¾‹
```bash
# ğŸš€ éƒ¨ç½²å‰å®Œæ•´æ£€æŸ¥æµç¨‹
echo "ğŸ” Pre-deployment verification..."
python3 tools/verify_structure.py && \
python3 scripts/debug/debug_device.py && \
python3 tools/config_generator.py --template production --export-env && \
echo "âœ… Ready for deployment!"

# ğŸ“Š ç”Ÿäº§ç¯å¢ƒç›‘æ§é…ç½®
# å¯åŠ¨å¥åº·ç›‘æ§ (åå°)
python3 tools/health_monitor.py \
  --mode monitor \
  --duration 86400 \
  --export daily_health.json \
  --alert-on-error &

# å¯åŠ¨æœåŠ¡
source configs/config_production.env
./scripts/start_service_general.sh

# ğŸ§ª å¼€å‘ç¯å¢ƒè°ƒè¯•æµç¨‹
echo "ğŸ”§ Development debug workflow..."
python3 scripts/debug/debug_memory.py --mode status
python3 scripts/debug/debug_t5_warmup.py --warmup-steps 1
python3 scripts/debug/debug_pipeline.py --mode comprehensive
echo "ğŸ¯ Debug completed!"
```

## ğŸ“Š æ€§èƒ½ç›‘æ§ä¸ä¼˜åŒ–

### T5 CPU æ¨¡å¼å¯¹æ¯”åˆ†æ

| é…ç½®æ¨¡å¼ | T5 ä½ç½® | æ˜¾å­˜å ç”¨ | ç”Ÿæˆæ—¶é—´ | å¹¶å‘èƒ½åŠ› | é€‚ç”¨åœºæ™¯ |
|----------|---------|----------|----------|----------|----------|
| **æ ‡å‡†æ¨¡å¼** | NPU/GPU | ~28GB | 2-3åˆ†é’Ÿ | 4-5ä»»åŠ¡ | å¤§æ˜¾å­˜ç¯å¢ƒ |
| **T5 CPU** | CPU | ~20GB | 2.5-3.5åˆ†é’Ÿ | 2-3ä»»åŠ¡ | æ˜¾å­˜å—é™ç¯å¢ƒ |
| **æ··åˆæ¨¡å¼** | è‡ªé€‚åº” | ~24GB | 2.2-3åˆ†é’Ÿ | 3-4ä»»åŠ¡ | å¹³è¡¡æ€§èƒ½ |
| **ä¼˜åŒ–æ¨¡å¼** | CPU+ç¼“å­˜ | ~18GB | 2.8-3.2åˆ†é’Ÿ | 3-4ä»»åŠ¡ | èŠ‚èƒ½é«˜æ•ˆ |

### è®¾å¤‡å¹³å°å¯¹æ¯”

| å¹³å° | ä¼˜åŠ¿ | åŠ£åŠ¿ | æ¨èé…ç½® | è°ƒè¯•é‡ç‚¹ |
|------|------|------|----------|----------|
| **NPU (æ˜‡è…¾)** | é«˜èƒ½æ•ˆã€å¤§æ˜¾å­˜ã€ç¨³å®š | ç”Ÿæ€ç›¸å¯¹è¾ƒæ–° | T5_CPU=true, ä¿å®ˆå¹¶å‘ | HCCLé€šä¿¡ã€å†…å­˜ç®¡ç† |
| **GPU (NVIDIA)** | æˆç†Ÿç”Ÿæ€ã€é«˜æ€§èƒ½ | åŠŸè€—è¾ƒé«˜ã€æ˜¾å­˜é™åˆ¶ | å¯é€‰T5_CPU=false | CUDAåŒæ­¥ã€æ˜¾å­˜ä¼˜åŒ– |

## ğŸ›ï¸ é…ç½®å‚æ•°

### ç¯å¢ƒå˜é‡é…ç½®

#### æ ¸å¿ƒé…ç½®

| å˜é‡å | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|------|
| `T5_CPU` | false | T5 ç¼–ç å™¨æ˜¯å¦ä½¿ç”¨ CPU |
| `DIT_FSDP` | true | DiT æ¨¡å‹æ˜¯å¦ä½¿ç”¨ FSDP åˆ†ç‰‡ |
| `T5_FSDP` | false | T5 ç¼–ç å™¨æ˜¯å¦ä½¿ç”¨ FSDP åˆ†ç‰‡ |
| `VAE_PARALLEL` | true | VAE æ˜¯å¦å¹¶è¡Œç¼–è§£ç  |
| `ULYSSES_SIZE` | 8 | Ulysses åºåˆ—å¹¶è¡Œç»„æ•° |

#### æœåŠ¡é…ç½®

| å˜é‡å | é»˜è®¤å€¼ | T5 CPU æ¨¡å¼é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|-------------------|------|
| `SERVER_HOST` | 0.0.0.0 | 0.0.0.0 | æœåŠ¡ç›‘å¬åœ°å€ |
| `SERVER_PORT` | 8088 | 8088 | æœåŠ¡ç«¯å£ |
| `MAX_CONCURRENT_TASKS` | 5 | 2 | æœ€å¤§å¹¶å‘ä»»åŠ¡æ•° |
| `TASK_TIMEOUT` | 1800 | 2400 | ä»»åŠ¡è¶…æ—¶æ—¶é—´(ç§’) |
| `CLEANUP_INTERVAL` | 300 | 300 | æ¸…ç†é—´éš”(ç§’) |
| `MAX_OUTPUT_DIR_SIZE` | 50 | 50 | æœ€å¤§è¾“å‡ºç›®å½•å¤§å°(GB) |
| `ALLOWED_HOSTS` | * | * | å…è®¸çš„ä¸»æœºåˆ—è¡¨ |
| `MODEL_CKPT_DIR` | /data/models/... | /data/models/... | æ¨¡å‹æ–‡ä»¶è·¯å¾„ |

#### é€šä¿¡é…ç½®

| å˜é‡å | é»˜è®¤å€¼ | T5 CPU æ¨¡å¼è°ƒæ•´ | è¯´æ˜ |
|--------|--------|-----------------|------|
| `HCCL_TIMEOUT` | 1800 | 2400 | HCCL é€šä¿¡è¶…æ—¶(ç§’) |
| `HCCL_CONNECT_TIMEOUT` | 600 | 900 | HCCL è¿æ¥è¶…æ—¶(ç§’) |
| `HCCL_BUFFSIZE` | 512 | 256 | HCCL ç¼“å†²åŒºå¤§å° |

#### NPU ä¸“ç”¨ç¯å¢ƒå˜é‡

| å˜é‡å | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|------|
| `ALGO` | 0 | NPU ç®—æ³•é€‰æ‹© |
| `PYTORCH_NPU_ALLOC_CONF` | expandable_segments:True | NPU å†…å­˜åˆ†é…ç­–ç•¥ |
| `TASK_QUEUE_ENABLE` | 2 | NPU ä»»åŠ¡é˜Ÿåˆ—å¯ç”¨çº§åˆ« |
| `CPU_AFFINITY_CONF` | 1 | CPU äº²å’Œæ€§é…ç½® |
| `ASCEND_LAUNCH_BLOCKING` | 0 | NPU å¯åŠ¨é˜»å¡æ¨¡å¼ |
| `ASCEND_GLOBAL_LOG_LEVEL` | 1 | NPU å…¨å±€æ—¥å¿—çº§åˆ« |
| `NPU_VISIBLE_DEVICES` | 0,1,2,3,4,5,6,7 | å¯è§ NPU è®¾å¤‡ |
| `ASCEND_RT_VISIBLE_DEVICES` | 0,1,2,3,4,5,6,7 | NPU è¿è¡Œæ—¶å¯è§è®¾å¤‡ |

#### CUDA ä¸“ç”¨ç¯å¢ƒå˜é‡

| å˜é‡å | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|--------|------|
| `NCCL_TIMEOUT` | 3600 | NCCL é€šä¿¡è¶…æ—¶(ç§’) |
| `CUDA_LAUNCH_BLOCKING` | 0 | CUDA å¯åŠ¨é˜»å¡æ¨¡å¼ |
| `NCCL_DEBUG` | INFO | NCCL è°ƒè¯•çº§åˆ« |
| `NCCL_IB_DISABLE` | 1 | ç¦ç”¨ InfiniBand |
| `NCCL_P2P_DISABLE` | 0 | ç¦ç”¨ç‚¹å¯¹ç‚¹é€šä¿¡ |
| `CUDA_VISIBLE_DEVICES` | 0,1,2,3,4,5,6,7 | å¯è§ CUDA è®¾å¤‡ |

#### CPU ä¼˜åŒ–é…ç½®

| å˜é‡å | é»˜è®¤å€¼ | T5 CPU æ¨¡å¼å»ºè®® | è¯´æ˜ |
|--------|--------|-----------------|------|
| `OMP_NUM_THREADS` | 8 | 16 | OpenMP çº¿ç¨‹æ•° |
| `MKL_NUM_THREADS` | 8 | 16 | MKL çº¿ç¨‹æ•° |
| `OPENBLAS_NUM_THREADS` | 8 | 16 | OpenBLAS çº¿ç¨‹æ•° |
| `TOKENIZERS_PARALLELISM` | false | false | åˆ†è¯å™¨å¹¶è¡Œå¤„ç† |

### API è¯·æ±‚å‚æ•°

#### åŸºç¡€å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | èŒƒå›´ | è¯´æ˜ |
|------|------|--------|------|------|
| `prompt` | string | - | 10-500å­—ç¬¦ | è§†é¢‘æè¿°æç¤ºè¯ |
| `image_url` | string | - | HTTP/HTTPS | è¾“å…¥å›¾åƒåœ°å€ |
| `image_size` | string | "1280*720" | æ”¯æŒçš„åˆ†è¾¨ç‡ | è¾“å‡ºè§†é¢‘åˆ†è¾¨ç‡ |
| `num_frames` | int | 81 | 24-120 | è§†é¢‘å¸§æ•° |
| `guidance_scale` | float | 3.0 | 1.0-20.0 | CFG å¼•å¯¼ç³»æ•° |
| `infer_steps` | int | 30 | 20-100 | å»å™ªæ¨ç†æ­¥æ•° |
| `seed` | int | null | 0-2147483647 | éšæœºæ•°ç§å­ |
| `negative_prompt` | string | null | 0-500å­—ç¬¦ | è´Ÿé¢æç¤ºè¯ |

#### åˆ†å¸ƒå¼å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `vae_parallel` | bool | false | VAE å¹¶è¡Œç¼–è§£ç  |
| `ulysses_size` | int | 1 | Ulysses åºåˆ—å¹¶è¡Œç»„æ•° |
| `dit_fsdp` | bool | false | DiT æ¨¡å‹ FSDP åˆ†ç‰‡ |
| `t5_fsdp` | bool | false | T5 ç¼–ç å™¨ FSDP åˆ†ç‰‡ |
| `cfg_size` | int | 1 | CFG å¹¶è¡Œç»„æ•° |

#### æ€§èƒ½ä¼˜åŒ–å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `use_attentioncache` | bool | false | å¯ç”¨æ³¨æ„åŠ›ç¼“å­˜ |
| `start_step` | int | 12 | ç¼“å­˜èµ·å§‹æ­¥æ•° |
| `attentioncache_interval` | int | 4 | ç¼“å­˜æ›´æ–°é—´éš” |
| `end_step` | int | 37 | ç¼“å­˜ç»“æŸæ­¥æ•° |
| `sample_solver` | string | "unipc" | é‡‡æ ·ç®—æ³• |
| `sample_shift` | float | 5.0 | é‡‡æ ·åç§» |

### è´¨é‡é¢„è®¾é…ç½®

#### é¢„è®¾æ¨¡å¼å¯¹æ¯”

| é¢„è®¾æ¨¡å¼ | æ¨ç†æ­¥æ•° | å¼•å¯¼ç³»æ•° | é‡‡æ ·ç®—æ³• | é€‚ç”¨åœºæ™¯ |
|----------|----------|----------|----------|----------|
| `fast` | 20 | 2.5 | unipc | å¿«é€Ÿæµ‹è¯• |
| `balanced` | 30 | 3.0 | unipc | æ—¥å¸¸ä½¿ç”¨ |
| `high` | 50 | 4.0 | dpmpp | é«˜è´¨é‡è¾“å‡º |
| `custom` | ç”¨æˆ·å®šä¹‰ | ç”¨æˆ·å®šä¹‰ | ç”¨æˆ·å®šä¹‰ | è‡ªå®šä¹‰é…ç½® |

#### æ”¯æŒçš„åˆ†è¾¨ç‡

| åˆ†è¾¨ç‡ | æ¯”ä¾‹ | æ˜¾å­˜éœ€æ±‚ | ç”Ÿæˆæ—¶é—´ | æ¨èåœºæ™¯ |
|--------|------|----------|----------|----------|
| `1280*720` | 16:9 | æ ‡å‡† | æ ‡å‡† | æ¨ªå±è§†é¢‘ |
| `720*1280` | 9:16 | æ ‡å‡† | æ ‡å‡† | ç«–å±è§†é¢‘ |
| `1024*576` | 16:9 | è¾ƒä½ | è¾ƒå¿« | å¿«é€Ÿé¢„è§ˆ |
| `1920*1080` | 16:9 | è¾ƒé«˜ | è¾ƒæ…¢ | é«˜æ¸…è¾“å‡º |

#### æ”¯æŒçš„å¸§æ•°

| å¸§æ•° | æ—¶é•¿ (24fps) | æ˜¾å­˜éœ€æ±‚ | ç”Ÿæˆæ—¶é—´ | æ¨èåœºæ™¯ |
|------|--------------|----------|----------|----------|
| 41 | ~1.7ç§’ | è¾ƒä½ | è¾ƒå¿« | çŸ­è§†é¢‘ç‰‡æ®µ |
| 61 | ~2.5ç§’ | æ ‡å‡† | æ ‡å‡† | å¸¸è§„è§†é¢‘ |
| 81 | ~3.4ç§’ | æ ‡å‡† | æ ‡å‡† | é»˜è®¤é•¿åº¦ |
| 121 | ~5.0ç§’ | è¾ƒé«˜ | è¾ƒæ…¢ | é•¿è§†é¢‘ |

### é…ç½®æ¨¡æ¿ç¤ºä¾‹

#### å¼€å‘ç¯å¢ƒé…ç½®

```bash
# å¼€å‘æµ‹è¯•ç¯å¢ƒ
export T5_CPU=true
export MAX_CONCURRENT_TASKS=1
export TASK_TIMEOUT=3600
export DIT_FSDP=false
export VAE_PARALLEL=false
export ULYSSES_SIZE=1
export ASCEND_GLOBAL_LOG_LEVEL=0  # è¯¦ç»†æ—¥å¿—
```

#### ç”Ÿäº§ç¯å¢ƒé…ç½®

```bash
# ç”Ÿäº§ç¯å¢ƒ - NPU é«˜æ€§èƒ½
export T5_CPU=false
export MAX_CONCURRENT_TASKS=5
export TASK_TIMEOUT=1800
export DIT_FSDP=true
export VAE_PARALLEL=true
export ULYSSES_SIZE=8
export USE_ATTENTION_CACHE=true
```

#### å†…å­˜ä¼˜åŒ–é…ç½®

```bash
# å†…å­˜å—é™ç¯å¢ƒ
export T5_CPU=true
export MAX_CONCURRENT_TASKS=2
export TASK_TIMEOUT=2400
export DIT_FSDP=true
export VAE_PARALLEL=false
export ULYSSES_SIZE=4
export HCCL_TIMEOUT=3600
```

#### è°ƒè¯•é…ç½®

```bash
# è°ƒè¯•æ¨¡å¼
export T5_CPU=true
export MAX_CONCURRENT_TASKS=1
export TASK_TIMEOUT=7200
export ASCEND_GLOBAL_LOG_LEVEL=0
export ASCEND_LAUNCH_BLOCKING=1
export PYTORCH_NPU_ALLOC_CONF="expandable_segments:True"
```

### é…ç½®ä¼˜å…ˆçº§

é…ç½®çš„åŠ è½½ä¼˜å…ˆçº§ï¼ˆä»é«˜åˆ°ä½ï¼‰ï¼š

1. **API è¯·æ±‚å‚æ•°** - å•æ¬¡è¯·æ±‚è¦†ç›–
2. **ç¯å¢ƒå˜é‡** - ç³»ç»Ÿçº§é…ç½®
3. **é…ç½®æ–‡ä»¶** - æŒä¹…åŒ–é…ç½®
4. **é»˜è®¤å€¼** - ä»£ç ä¸­çš„é»˜è®¤é…ç½®

### é…ç½®éªŒè¯å’Œç”Ÿæˆ

#### è‡ªåŠ¨é…ç½®ç”Ÿæˆ
```bash
# ğŸ¤– æ™ºèƒ½ç¡¬ä»¶æ£€æµ‹é…ç½®ç”Ÿæˆ
python3 tools/config_generator.py --template production --export-env

# ğŸ¯ é’ˆå¯¹ä¸åŒåœºæ™¯ç”Ÿæˆé…ç½®
python3 tools/config_generator.py --template development --export-env --output-dir configs/
python3 tools/config_generator.py --template testing --export-env --output-dir configs/
python3 tools/config_generator.py --template memory_efficient --export-env --output-dir configs/

# ğŸ”§ è‡ªå®šä¹‰é…ç½®ç”Ÿæˆ
python3 tools/config_generator.py \
  --template production \
  --custom '{"t5_cpu": true, "max_concurrent_tasks": 3, "use_attentioncache": true}' \
  --model-path /custom/model/path \
  --port 8080 \
  --export-env \
  --export-json
```

#### é…ç½®éªŒè¯
```bash
# ğŸ“‹ éªŒè¯é…ç½®å®Œæ•´æ€§
python3 tools/verify_structure.py

# ğŸ” é…ç½®ç¯å¢ƒæ£€æŸ¥
python3 scripts/debug/debug_device.py

# âš™ï¸ ç”Ÿæˆé…ç½®å¯¹æ¯”
echo "ğŸ” Configuration comparison:"
echo "Development vs Production:"
diff configs/config_development.env configs/config_production.env

echo "Memory Efficient vs High Performance:"
diff configs/config_memory_efficient.env configs/config_production.env
```

### åŠ¨æ€é…ç½®è°ƒæ•´

#### è¿è¡Œæ—¶é…ç½®æŸ¥çœ‹
```bash
# ğŸ¥ æŸ¥çœ‹å½“å‰æœåŠ¡é…ç½®
curl http://localhost:8088/health | jq '.config'

# ğŸ“Š æŸ¥çœ‹è®¾å¤‡ä¿¡æ¯
curl http://localhost:8088/device-info | jq '.'

# ğŸ“ˆ æŸ¥çœ‹æ€§èƒ½æŒ‡æ ‡
curl http://localhost:8088/metrics | jq '.performance'
```

#### é…ç½®çƒ­æ›´æ–°ï¼ˆéƒ¨åˆ†æ”¯æŒï¼‰
```bash
# âš¡ éƒ¨åˆ†é…ç½®æ”¯æŒçƒ­æ›´æ–°ï¼ˆéœ€è¦é‡å¯ç”Ÿæ•ˆçš„é…ç½®ï¼‰
# T5_CPU, DIT_FSDP, VAE_PARALLEL - éœ€è¦é‡å¯
# MAX_CONCURRENT_TASKS, TASK_TIMEOUT - æ”¯æŒçƒ­æ›´æ–°

# ğŸ“ æ›´æ–°å¹¶å‘é…ç½®ï¼ˆç¤ºä¾‹ï¼‰
export MAX_CONCURRENT_TASKS=3
curl -X POST http://localhost:8088/admin/update-config \
  -H "Content-Type: application/json" \
  -d '{"max_concurrent_tasks": 3, "task_timeout": 2400}'
```

### é…ç½®æœ€ä½³å®è·µ

#### ğŸ¯ ç¯å¢ƒåˆ†ç¦»
```bash
# ğŸ  å¼€å‘ç¯å¢ƒ
source configs/config_development.env
./scripts/start_service_general.sh

# ğŸš€ ç”Ÿäº§ç¯å¢ƒ  
source configs/config_production.env
./scripts/start_service_general.sh

# ğŸ§ª æµ‹è¯•ç¯å¢ƒ
source configs/config_testing.env
./scripts/start_service_general.sh
```

#### ğŸ”’ å®‰å…¨é…ç½®
```bash
# ğŸ›¡ï¸ ç”Ÿäº§å®‰å…¨é…ç½®
export ALLOWED_HOSTS="api.company.com,*.company.com"
export CORS_ORIGINS="https://app.company.com"
export MAX_REQUEST_SIZE="50M"
export RATE_LIMIT_REQUESTS=100
export RATE_LIMIT_WINDOW=3600

# ğŸ“ æ–‡ä»¶æƒé™
export OUTPUT_DIR_PERMISSIONS=750
export LOG_DIR_PERMISSIONS=640
```

#### ğŸ“Š æ€§èƒ½è°ƒä¼˜é…ç½®
```bash
# âš¡ é«˜æ€§èƒ½é…ç½® (å¤§æ˜¾å­˜ç¯å¢ƒ)
export T5_CPU=false                    # T5 ä½¿ç”¨ GPU
export VAE_PARALLEL=true               # VAE å¹¶è¡Œ
export ULYSSES_SIZE=8                  # æœ€å¤§åºåˆ—å¹¶è¡Œ
export USE_ATTENTION_CACHE=true        # æ³¨æ„åŠ›ç¼“å­˜
export MAX_CONCURRENT_TASKS=5          # é«˜å¹¶å‘

# ğŸ§  å†…å­˜ä¼˜åŒ–é…ç½® (æ˜¾å­˜å—é™ç¯å¢ƒ)
export T5_CPU=true                     # T5 ä½¿ç”¨ CPU
export VAE_PARALLEL=false              # å…³é—­ VAE å¹¶è¡Œ
export ULYSSES_SIZE=4                  # é€‚ä¸­åºåˆ—å¹¶è¡Œ
export MAX_CONCURRENT_TASKS=2          # ä¿å®ˆå¹¶å‘
export DIT_FSDP=true                   # å¯ç”¨æ¨¡å‹åˆ†ç‰‡
```

### é…ç½®æ•…éšœæ’é™¤

#### ğŸ” é…ç½®è¯Šæ–­
```bash
# ğŸ“‹ é…ç½®æ£€æŸ¥æ¸…å•
echo "ğŸ” Configuration Diagnostics"
echo "=========================="

# 1. ç¯å¢ƒå˜é‡æ£€æŸ¥
echo "1. Environment Variables:"
env | grep -E "(T5_CPU|DIT_FSDP|MAX_CONCURRENT|MODEL_CKPT_DIR)" | sort

# 2. è®¾å¤‡é…ç½®æ£€æŸ¥
echo "2. Device Configuration:"
python3 -c "
from utils.device_detector import device_detector
device_type, device_count = device_detector.detect_device()
print(f'Device: {device_type.value} x {device_count}')
"

# 3. å†…å­˜é…ç½®æ£€æŸ¥
echo "3. Memory Configuration:"
python3 scripts/debug/debug_memory.py --mode status

# 4. æ¨¡å‹è·¯å¾„æ£€æŸ¥
echo "4. Model Path:"
if [ -d "$MODEL_CKPT_DIR" ]; then
    echo "âœ… Model directory exists: $MODEL_CKPT_DIR"
    ls -la "$MODEL_CKPT_DIR" | head -5
else
    echo "âŒ Model directory not found: $MODEL_CKPT_DIR"
fi
```

#### âš ï¸ å¸¸è§é…ç½®é—®é¢˜

| é—®é¢˜ | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| T5 CPU å†…å­˜ä¸è¶³ | T5 åŠ è½½å¤±è´¥ | å¢åŠ ç³»ç»Ÿå†…å­˜æˆ–ä½¿ç”¨ T5 GPU æ¨¡å¼ |
| å¹¶å‘ä»»åŠ¡è¿‡å¤š | GPU OOM | é™ä½ MAX_CONCURRENT_TASKS |
| é€šä¿¡è¶…æ—¶ | åˆ†å¸ƒå¼è®­ç»ƒå¤±è´¥ | å¢åŠ  HCCL_TIMEOUT/NCCL_TIMEOUT |
| æ¨¡å‹è·¯å¾„é”™è¯¯ | æ¨¡å‹åŠ è½½å¤±è´¥ | æ£€æŸ¥ MODEL_CKPT_DIR è·¯å¾„ |
| ç«¯å£è¢«å ç”¨ | æœåŠ¡å¯åŠ¨å¤±è´¥ | ä¿®æ”¹ SERVER_PORT æˆ–é‡Šæ”¾ç«¯å£ |

è¿™æ ·é…ç½®å‚æ•°éƒ¨åˆ†å°±å®Œæ•´äº†ï¼ŒåŒ…å«äº†æ‰€æœ‰é‡è¦çš„é…ç½®é€‰é¡¹ã€ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µï¼

## ğŸ“š API æ¥å£æ–‡æ¡£

### æ ¸å¿ƒæ¥å£æ€»è§ˆ

| ç«¯ç‚¹ | æ–¹æ³• | åŠŸèƒ½ | ç›‘æ§æŒ‡æ ‡ |
|------|------|------|----------|
| `/video/submit` | POST | æäº¤è§†é¢‘ç”Ÿæˆä»»åŠ¡ | è¯·æ±‚é‡ã€æˆåŠŸç‡ã€é˜Ÿåˆ—é•¿åº¦ |
| `/video/status` | POST | æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ | æŸ¥è¯¢é¢‘ç‡ã€å“åº”æ—¶é—´ |
| `/video/cancel` | POST | å–æ¶ˆä»»åŠ¡ | å–æ¶ˆç‡ã€èµ„æºå›æ”¶æ—¶é—´ |
| `/health` | GET | æœåŠ¡å¥åº·æ£€æŸ¥ | å¥åº·çŠ¶æ€ã€èµ„æºä½¿ç”¨ç‡ |
| `/metrics` | GET | ç›‘æ§æŒ‡æ ‡ | ç³»ç»Ÿæ€§èƒ½ã€ä»»åŠ¡ç»Ÿè®¡ |
| `/device-info` | GET | è®¾å¤‡ä¿¡æ¯ | è®¾å¤‡çŠ¶æ€ã€é…ç½®ä¿¡æ¯ |

### è¯·æ±‚å‚æ•°åˆ†å±‚è®¾è®¡

#### ğŸ¯ ç®€åŒ–æ¨¡å¼ (æ¨èæ™®é€šç”¨æˆ·)
```json
{
  "prompt": "A cat playing in the garden",
  "image_url": "https://example.com/input.jpg",
  "quality_preset": "balanced"
}
```

#### ğŸ”§ é«˜çº§æ¨¡å¼ (ä¸“ä¸šç”¨æˆ·)
```json
{
  "prompt": "A cat playing in the garden",
  "image_url": "https://example.com/input.jpg", 
  "quality_preset": "custom",
  "guidance_scale": 4.0,
  "infer_steps": 40,
  "num_frames": 81,
  "image_size": "1280*720"
}
```

#### âš™ï¸ ä¸“å®¶æ¨¡å¼ (ç³»ç»Ÿç®¡ç†å‘˜)
```json
{
  "prompt": "A cat playing in the garden",
  "image_url": "https://example.com/input.jpg",
  "guidance_scale": 4.0,
  "infer_steps": 40,
  "t5_fsdp": true,
  "vae_parallel": true,
  "use_attentioncache": true,
  "cache_start_step": 12,
  "ulysses_size": 8
}
```

### å¢å¼ºç‰ˆå¥åº·æ£€æŸ¥å“åº”
```json
{
  "status": "healthy",
  "timestamp": 1703847600.123,
  "uptime": 3600.5,
  "config": {
    "device_type": "npu",
    "device_count": 8,
    "t5_cpu": true,
    "max_concurrent": 2,
    "model_path": "/data/models/...",
    "backend": "torch_npu"
  },
  "service": {
    "total_tasks": 15,
    "active_tasks": 1,
    "completed_tasks": 13,
    "failed_tasks": 1,
    "success_rate": 92.3,
    "avg_generation_time": 165.2
  },
  "resources": {
    "memory_usage": "68.5%",
    "device_memory_usage": "45.2%",
    "cpu_usage": "23.1%",
    "available_slots": 1,
    "disk_usage": "12.5GB"
  },
  "performance": {
    "requests_per_minute": 2.3,
    "avg_response_time": 2.1,
    "cache_hit_rate": 78.5
  }
}
```

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹ä¸æœ€ä½³å®è·µ

### å®Œæ•´çš„å®¢æˆ·ç«¯ç¤ºä¾‹

```python
import requests
import time
import json
from typing import Optional, Dict, Any

class I2VClient:
    """æ™ºèƒ½ I2V API å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = "http://localhost:8088"):
        self.base_url = base_url
        self.session = requests.Session()
    
    def check_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        response = self.session.get(f"{self.base_url}/health")
        return response.json()
    
    def get_optimal_config(self) -> Dict[str, Any]:
        """è·å–å½“å‰æœ€ä¼˜é…ç½®å»ºè®®"""
        health = self.check_health()
        
        # æ ¹æ®æœåŠ¡çŠ¶æ€æ¨èé…ç½®
        config = {
            "guidance_scale": 3.0,
            "infer_steps": 30,
            "num_frames": 81
        }
        
        # æ ¹æ®è´Ÿè½½è°ƒæ•´
        if health["service"]["active_tasks"] >= health["config"]["max_concurrent"] * 0.8:
            config["infer_steps"] = 25  # é™ä½æ­¥æ•°æé«˜åå
        
        # æ ¹æ®T5æ¨¡å¼è°ƒæ•´
        if health["config"]["t5_cpu"]:
            config["guidance_scale"] = 3.5  # CPUæ¨¡å¼å¯ä»¥ç¨é«˜
        
        return config
    
    def submit_video(self, 
                    prompt: str, 
                    image_url: str,
                    auto_optimize: bool = True,
                    **kwargs) -> str:
        """æ™ºèƒ½æäº¤è§†é¢‘ç”Ÿæˆä»»åŠ¡"""
        
        # åŸºç¡€è¯·æ±‚
        request_data = {
            "prompt": prompt,
            "image_url": image_url,
        }
        
        # è‡ªåŠ¨ä¼˜åŒ–é…ç½®
        if auto_optimize:
            optimal_config = self.get_optimal_config()
            request_data.update(optimal_config)
        
        # ç”¨æˆ·è‡ªå®šä¹‰è¦†ç›–
        request_data.update(kwargs)
        
        # æäº¤ä»»åŠ¡
        response = self.session.post(
            f"{self.base_url}/video/submit",
            json=request_data
        )
        
        if response.status_code == 429:
            raise Exception("æœåŠ¡å™¨ç¹å¿™ï¼Œè¯·ç¨åé‡è¯•")
        elif response.status_code != 200:
            raise Exception(f"æäº¤å¤±è´¥: {response.text}")
        
        return response.json()["requestId"]
    
    def wait_for_completion(self, 
                          task_id: str, 
                          timeout: int = 3600,
                          progress_callback: Optional[callable] = None) -> Dict[str, Any]:
        """ç­‰å¾…ä»»åŠ¡å®Œæˆ"""
        
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            # æŸ¥è¯¢çŠ¶æ€
            status_response = self.session.post(
                f"{self.base_url}/video/status",
                json={"requestId": task_id}
            )
            
            status_data = status_response.json()
            
            # è¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(status_data)
            
            # æ£€æŸ¥å®ŒæˆçŠ¶æ€
            if status_data["status"] == "Succeed":
                return status_data
            elif status_data["status"] == "Failed":
                raise Exception(f"ä»»åŠ¡å¤±è´¥: {status_data.get('reason', 'æœªçŸ¥åŸå› ')}")
            
            # åŠ¨æ€è°ƒæ•´è½®è¯¢é—´éš”
            elapsed = time.time() - start_time
            if elapsed < 30:
                interval = 2  # å‰30ç§’å¯†é›†è½®è¯¢
            elif elapsed < 120:
                interval = 5  # 2åˆ†é’Ÿå†…ä¸­ç­‰é¢‘ç‡
            else:
                interval = 10  # ä¹‹åä½é¢‘è½®è¯¢
            
            time.sleep(interval)
        
        raise Exception(f"ä»»åŠ¡è¶…æ—¶: {timeout}ç§’")

# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    """å®Œæ•´ä½¿ç”¨ç¤ºä¾‹"""
    
    client = I2VClient("http://localhost:8088")
    
    try:
        # 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€
        health = client.check_health()
        print(f"ğŸ¥ Service health: {health['status']}")
        print(f"ğŸ¯ Device: {health['config']['device_type']} x {health['config']['device_count']}")
        print(f"ğŸ§  T5 CPU mode: {health['config']['t5_cpu']}")
        print(f"ğŸ“Š Active tasks: {health['service']['active_tasks']}/{health['config']['max_concurrent']}")
        
        # 2. æ™ºèƒ½æäº¤ä»»åŠ¡
        task_id = client.submit_video(
            prompt="A serene lake with a swan gracefully swimming",
            image_url="https://picsum.photos/1280/720",
            auto_optimize=True,  # è‡ªåŠ¨ä¼˜åŒ–é…ç½®
            quality_preset="high"  # ç”¨æˆ·åå¥½
        )
        
        print(f"ğŸš€ Task submitted: {task_id}")
        
        # 3. ç­‰å¾…å®Œæˆ (å¸¦è¿›åº¦æ˜¾ç¤º)
        def progress_callback(status):
            print(f"ğŸ“Š Status: {status['status']} - {status.get('message', '')}")
        
        result = client.wait_for_completion(
            task_id, 
            timeout=3600,
            progress_callback=progress_callback
        )
        
        print(f"âœ… Video completed: {result['results']['video_url']}")
        print(f"â±ï¸  Generation time: {result['results']['generation_time']:.1f}s")
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")

if __name__ == "__main__":
    example_usage()
```

### æ‰¹é‡å¤„ç†ç¤ºä¾‹

```python
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor

async def batch_process_videos(prompts_and_images: list, max_concurrent: int = 3):
    """å¼‚æ­¥æ‰¹é‡å¤„ç†è§†é¢‘"""
    
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def process_single_video(prompt, image_url):
        async with semaphore:
            async with aiohttp.ClientSession() as session:
                # æäº¤ä»»åŠ¡
                async with session.post(
                    "http://localhost:8088/video/submit",
                    json={
                        "prompt": prompt,
                        "image_url": image_url,
                        "quality_preset": "balanced"
                    }
                ) as response:
                    if response.status != 200:
                        return {"error": await response.text()}
                    
                    task_data = await response.json()
                    task_id = task_data["requestId"]
                
                # ç­‰å¾…å®Œæˆ
                while True:
                    async with session.post(
                        "http://localhost:8088/video/status",
                        json={"requestId": task_id}
                    ) as status_response:
                        status_data = await status_response.json()
                        
                        if status_data["status"] == "Succeed":
                            return {
                                "task_id": task_id,
                                "video_url": status_data["results"]["video_url"],
                                "generation_time": status_data["results"]["generation_time"]
                            }
                        elif status_data["status"] == "Failed":
                            return {"error": status_data.get("reason", "Unknown error")}
                    
                    await asyncio.sleep(5)
    
    # å¹¶å‘å¤„ç†
    tasks = [
        process_single_video(prompt, image_url) 
        for prompt, image_url in prompts_and_images
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results

# ä½¿ç”¨ç¤ºä¾‹
async def batch_example():
    tasks = [
        ("A cat playing with a ball", "https://example.com/cat.jpg"),
        ("A dog running in the park", "https://example.com/dog.jpg"),
        ("A bird flying in the sky", "https://example.com/bird.jpg"),
    ]
    
    results = await batch_process_videos(tasks, max_concurrent=2)
    
    for i, result in enumerate(results):
        if isinstance(result, dict) and "error" not in result:
            print(f"âœ… Task {i+1}: {result['video_url']} ({result['generation_time']:.1f}s)")
        else:
            print(f"âŒ Task {i+1}: {result.get('error', str(result))}")

# asyncio.run(batch_example())
```

## ğŸ› ï¸ æ•…éšœæ’é™¤ä¸è¿ç»´

### å¸¸è§é—®é¢˜è¯Šæ–­æµç¨‹

#### ğŸ” å¯åŠ¨å¤±è´¥è¯Šæ–­
```bash
# 1. é¡¹ç›®ç»“æ„æ£€æŸ¥
echo "ğŸ“‹ Checking project structure..."
python3 tools/verify_structure.py

# 2. è®¾å¤‡ç¯å¢ƒæ£€æŸ¥  
echo "ğŸ”§ Checking device environment..."
python3 scripts/debug/debug_device.py

# 3. ä¾èµ–æ£€æŸ¥
echo "ğŸ“¦ Checking dependencies..."
python3 -c "
try:
    import torch, fastapi, uvicorn
    from schemas import VideoSubmitRequest
    from pipelines import PipelineFactory
    from utils import device_detector
    print('âœ… All imports successful')
except ImportError as e:
    print(f'âŒ Import failed: {e}')
"

# 4. æ¨¡å‹è·¯å¾„æ£€æŸ¥
echo "ğŸ“ Checking model path..."
if [ -d "$MODEL_CKPT_DIR" ]; then
    echo "âœ… Model directory exists: $MODEL_CKPT_DIR"
    ls -la "$MODEL_CKPT_DIR" | head -10
else
    echo "âŒ Model directory not found: $MODEL_CKPT_DIR"
fi
```

#### ğŸ§  å†…å­˜é—®é¢˜è¯Šæ–­
```bash
# 1. å½“å‰å†…å­˜çŠ¶æ€
python3 scripts/debug/debug_memory.py --mode status

# 2. T5 CPU æ¨¡å¼æ£€æŸ¥
if [ "$T5_CPU" = "true" ]; then
    echo "âœ… T5 CPU mode enabled"
    python3 scripts/debug/debug_t5_warmup.py --warmup-steps 1
else
    echo "âš ï¸  T5 GPU mode - high memory usage expected"
fi

# 3. å»ºè®®çš„å†…å­˜ä¼˜åŒ–é…ç½®
echo "ğŸ’¡ Recommended memory optimization:"
echo "   T5_CPU=true"
echo "   MAX_CONCURRENT_TASKS=2"
echo "   DIT_FSDP=true"
echo "   VAE_PARALLEL=false"
```

#### ğŸ”— é€šä¿¡é—®é¢˜è¯Šæ–­
```bash
# NPU é€šä¿¡æ£€æŸ¥
if [ "$device_type" = "npu" ]; then
    echo "ğŸ” NPU communication check..."
    python3 -c "
import torch_npu
print(f'NPU available: {torch_npu.npu.is_available()}')
print(f'NPU count: {torch_npu.npu.device_count()}')
try:
    torch_npu.npu.synchronize()
    print('âœ… NPU synchronization OK')
except Exception as e:
    print(f'âŒ NPU sync failed: {e}')
"
fi

# GPU é€šä¿¡æ£€æŸ¥
if [ "$device_type" = "cuda" ]; then
    echo "ğŸ” CUDA communication check..."
    python3 -c "
import torch
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'CUDA count: {torch.cuda.device_count()}')
try:
    torch.cuda.synchronize()
    print('âœ… CUDA synchronization OK')
except Exception as e:
    print(f'âŒ CUDA sync failed: {e}')
"
fi
```

### è‡ªåŠ¨æ•…éšœæ¢å¤è„šæœ¬

```bash
#!/bin/bash
# ğŸ“§ auto_recovery.sh - è‡ªåŠ¨æ•…éšœæ¢å¤è„šæœ¬

echo "ğŸš¨ Auto Recovery Script Started"

# 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€
if ! curl -s http://localhost:8088/health > /dev/null; then
    echo "âŒ Service not responding, attempting recovery..."
    
    # åœæ­¢æ—§è¿›ç¨‹
    pkill -f "i2v_api.py" || true
    pkill -f "torchrun.*i2v_api" || true
    sleep 10
    
    # æ¸…ç†è®¾å¤‡ç¼“å­˜
    python3 -c "
try:
    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print('âœ… CUDA cache cleared')
except: pass

try:
    import torch_npu
    if torch_npu.npu.is_available():
        torch_npu.npu.empty_cache()
        print('âœ… NPU cache cleared')
except: pass
"
    
    # é‡å¯æœåŠ¡
    echo "ğŸ”„ Restarting service..."
    ./scripts/start_service_general.sh &
    
    # ç­‰å¾…å¯åŠ¨
    sleep 60
    
    # éªŒè¯æ¢å¤
    if curl -s http://localhost:8088/health > /dev/null; then
        echo "âœ… Service recovered successfully"
    else
        echo "âŒ Recovery failed, manual intervention required"
        exit 1
    fi
else
    echo "âœ… Service is healthy"
fi

# 2. æ£€æŸ¥èµ„æºä½¿ç”¨
python3 scripts/debug/debug_memory.py --mode status

# 3. æ¸…ç†æ—§æ–‡ä»¶ (å¯é€‰)
find generated_videos/ -name "*.mp4" -mtime +7 -delete
find logs/ -name "*.log" -mtime +7 -delete

echo "ğŸ¯ Auto recovery completed"
```

### ç”Ÿäº§ç¯å¢ƒç›‘æ§é…ç½®

```bash
#!/bin/bash
# ğŸ” production_monitor.sh - ç”Ÿäº§ç¯å¢ƒç›‘æ§

# åˆ›å»ºç›‘æ§ç›®å½•
mkdir -p monitoring/{logs,alerts,reports}

# 1. å¥åº·ç›‘æ§ (24/7)
python3 tools/health_monitor.py \
  --mode monitor \
  --duration 86400 \
  --interval 300 \
  --export monitoring/logs/health_$(date +%Y%m%d).json \
  --alert-on-error \
  --alert-threshold-memory 85 \
  --alert-threshold-response-time 30 &

# 2. å†…å­˜ç›‘æ§
python3 scripts/debug/debug_memory.py \
  --mode monitor \
  --duration 86400 \
  --interval 600 \
  --export monitoring/logs/memory_$(date +%Y%m%d).csv &

# 3. æ€§èƒ½åŸºå‡† (æ¯å°æ—¶)
while true; do
    python3 tools/benchmark.py \
      --response-test 5 \
      --export monitoring/reports/benchmark_$(date +%Y%m%d_%H%M).json
    sleep 3600
done &

# 4. è‡ªåŠ¨æ¢å¤æ£€æŸ¥ (æ¯30åˆ†é’Ÿ)
while true; do
    ./auto_recovery.sh >> monitoring/logs/recovery_$(date +%Y%m%d).log 2>&1
    sleep 1800
done &

echo "ğŸš€ Production monitoring started"
echo "ğŸ“Š Logs: monitoring/logs/"
echo "ğŸ“ˆ Reports: monitoring/reports/"
```

## ğŸ”’ ç”Ÿäº§éƒ¨ç½²æœ€ä½³å®è·µ

### å®‰å…¨é…ç½®
```bash
# è®¿é—®æ§åˆ¶
export ALLOWED_HOSTS="api.company.com,*.company.com"
export CORS_ORIGINS="https://app.company.com,https://admin.company.com"

# èµ„æºé™åˆ¶
export MAX_CONCURRENT_TASKS=3
export TASK_TIMEOUT=1800
export MAX_OUTPUT_DIR_SIZE=100
export MAX_REQUEST_SIZE=50M

# API é™æµ
export RATE_LIMIT_REQUESTS=100
export RATE_LIMIT_WINDOW=3600

# æ–‡ä»¶æƒé™
chmod 750 generated_videos/
chown i2v-service:i2v-service generated_videos/
```

### Docker éƒ¨ç½²é…ç½®
```dockerfile
# Dockerfile
FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel

WORKDIR /app

# ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Python ä¾èµ–
COPY requirements.txt .
RUN pip install -r requirements.txt

# é¡¹ç›®æ–‡ä»¶
COPY . .

# æƒé™è®¾ç½®
RUN chmod +x scripts/*.sh

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
  CMD curl -f http://localhost:8088/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["./scripts/start_service_general.sh"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  i2v-api:
    build: .
    ports:
      - "8088:8088"
    environment:
      - T5_CPU=true
      - MAX_CONCURRENT_TASKS=2
      - MODEL_CKPT_DIR=/data/models
    volumes:
      - ./generated_videos:/app/generated_videos
      - ./logs:/app/logs
      - /data/models:/data/models:ro
    deploy:
      resources:
        limits:
          memory: 64G
        reservations:
          memory: 32G
    restart: unless-stopped
    
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./generated_videos:/var/www/videos:ro
    depends_on:
      - i2v-api
    restart: unless-stopped
```

### Kubernetes éƒ¨ç½²é…ç½®
```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: i2v-api
spec:
  replicas: 2
  selector:
    matchLabels:
      app: i2v-api
  template:
    metadata:
      labels:
        app: i2v-api
    spec:
      containers:
      - name: i2v-api
        image: your-registry/i2v-api:latest
        ports:
        - containerPort: 8088
        env:
        - name: T5_CPU
          value: "true"
        - name: MAX_CONCURRENT_TASKS
          value: "2"
        resources:
          requests:
            memory: "32Gi"
            nvidia.com/gpu: 4
          limits:
            memory: "64Gi"
            nvidia.com/gpu: 4
        livenessProbe:
          httpGet:
            path: /health
            port: 8088
          initialDelaySeconds: 120
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8088
          initialDelaySeconds: 60
          periodSeconds: 10
```

## ğŸ“‹ ä¾èµ–æ¸…å•

### æ ¸å¿ƒä¾èµ–
```txt
# API æ¡†æ¶
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.5.0

# æ·±åº¦å­¦ä¹ 
torch>=2.0.0
transformers>=4.30.0
diffusers>=0.20.0

# å›¾åƒå¤„ç†
Pillow>=10.0.0
opencv-python>=4.8.0

# æ•°å€¼è®¡ç®—
numpy>=1.24.0

# HTTP å®¢æˆ·ç«¯
aiohttp>=3.9.0
requests>=2.31.0

# ç³»ç»Ÿç›‘æ§
psutil>=5.9.0
```

### è®¾å¤‡ç‰¹å®šä¾èµ–
```txt
# NPU ç¯å¢ƒ (åä¸ºæ˜‡è…¾)
torch_npu  # æ ¹æ®CANNç‰ˆæœ¬é€‰æ‹©

# GPU ç¯å¢ƒ (NVIDIA)
torch[cuda]>=2.0.0

# å¼€å‘å·¥å…·
pytest>=7.4.0
black>=23.0.0
isort>=5.12.0
mypy>=1.5.0
```

### å¯é€‰å¢å¼ºä¾èµ–
```txt
# æ€§èƒ½ç›‘æ§
prometheus-client>=0.18.0
grafana-api>=1.0.3

# æ—¥å¿—ç®¡ç†
structlog>=23.1.0
python-json-logger>=2.0.7

# é…ç½®ç®¡ç†
python-dotenv>=1.0.0
pyyaml>=6.0.1

# å¼€å‘è°ƒè¯•
ipython>=8.14.0
jupyter>=1.0.0
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

### å¼€å‘ç¯å¢ƒæ­å»º
```bash
# 1. Fork å¹¶å…‹éš†é¡¹ç›®
git clone https://github.com/your-username/fastapi-multigpu-i2v.git
cd fastapi-multigpu-i2v

# 2. åˆ›å»ºå¼€å‘åˆ†æ”¯
git checkout -b feature/your-amazing-feature

# 3. å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements.txt
pip install -r requirements-dev.txt

# 4. éªŒè¯å¼€å‘ç¯å¢ƒ
python3 tools/verify_structure.py
python3 scripts/debug/debug_device.py

# 5. è¿è¡Œæµ‹è¯•
python3 -m pytest tests/ -v
```

### ä»£ç è´¨é‡æ£€æŸ¥
```bash
# ä»£ç æ ¼å¼åŒ–
black src/ tools/ scripts/debug/
isort src/ tools/ scripts/debug/

# ç±»å‹æ£€æŸ¥
mypy src/

# æµ‹è¯•è¦†ç›–ç‡
pytest --cov=src --cov-report=html tests/

# ä»£ç è´¨é‡
flake8 src/ tools/
pylint src/ tools/
```

### æäº¤è§„èŒƒ
```bash
# æäº¤å‰æ£€æŸ¥
python3 tools/verify_structure.py
python3 -m pytest tests/
black --check src/ tools/

# æäº¤æ ¼å¼
git commit -m "feat: add amazing feature

- Add new functionality X
- Improve performance by Y%
- Fix issue #123

Closes #123"
```

## ğŸ“ æŠ€æœ¯æ”¯æŒä¸ç¤¾åŒº

### é—®é¢˜åé¦ˆæ¸ é“
- **ğŸ› Bug æŠ¥å‘Š**: [GitHub Issues](https://github.com/your-repo/issues)
- **ğŸ’¡ åŠŸèƒ½å»ºè®®**: [GitHub Discussions](https://github.com/your-repo/discussions)
- **ğŸ“– æ–‡æ¡£é—®é¢˜**: [Documentation Issues](https://github.com/your-repo/issues?q=is%3Aissue+label%3Adocumentation)

### æŠ€æœ¯æ–‡æ¡£
- **ğŸ“š è¯¦ç»†æ–‡æ¡£**: [é¡¹ç›® Wiki](https://github.com/your-repo/wiki)
- **ğŸ¥ è§†é¢‘æ•™ç¨‹**: [YouTube Channel](https://youtube.com/your-channel)
- **ğŸ“Š æ€§èƒ½æŠ¥å‘Š**: [Benchmark Results](https://github.com/your-repo/wiki/benchmarks)

### ç¤¾åŒºæ”¯æŒ
- **ğŸ’¬ å³æ—¶è®¨è®º**: [Discord Server](https://discord.gg/your-server)
- **ğŸ“§ é‚®ä»¶åˆ—è¡¨**: [Google Groups](https://groups.google.com/your-group)
- **ğŸŒ ä¸­æ–‡ç¤¾åŒº**: [å›½å†…æŠ€æœ¯è®ºå›](https://forum.your-site.com)

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- **ğŸ§  Wan AI Team** - æä¾› Wan2.1-I2V-14B-720P åŸºç¡€æ¨¡å‹
- **ğŸ”¥ åä¸ºæ˜‡è…¾** - NPU ç¡¬ä»¶æ”¯æŒå’Œ CANN è½¯ä»¶æ ˆ
- **ğŸ’š NVIDIA** - GPU ç¡¬ä»¶æ”¯æŒå’Œ CUDA ç”Ÿæ€ç³»ç»Ÿ
- **âš¡ FastAPI Team** - é«˜æ€§èƒ½å¼‚æ­¥ Web æ¡†æ¶
- **ğŸ”¥ PyTorch Team** - å¼ºå¤§çš„æ·±åº¦å­¦ä¹ æ¡†æ¶
- **ğŸŒŸ å¼€æºç¤¾åŒº** - å„ç§ä¼˜ç§€çš„å¼€æºé¡¹ç›®å’Œå·¥å…·

---

## ğŸ¯ å¿«é€ŸéªŒè¯ä¸å¯åŠ¨

### ä¸€é”®éªŒè¯è„šæœ¬
```bash
#!/bin/bash
# ğŸš€ quick_start.sh - ä¸€é”®éªŒè¯å’Œå¯åŠ¨

echo "ğŸ¯ FastAPI Multi-GPU I2V - Quick Start"
echo "====================================="

# 1. é¡¹ç›®éªŒè¯
echo "1ï¸âƒ£ Verifying project structure..."
python3 tools/verify_structure.py || exit 1

# 2. è®¾å¤‡æ£€æµ‹
echo "2ï¸âƒ£ Detecting hardware..."
python3 scripts/debug/debug_device.py || exit 1

# 3. ç”Ÿæˆé…ç½®
echo "3ï¸âƒ£ Generating optimal configuration..."
python3 tools/config_generator.py --template production --export-env --output-dir .

# 4. é¢„çƒ­æµ‹è¯•
echo "4ï¸âƒ£ Running warmup test..."
python3 scripts/debug/debug_t5_warmup.py --warmup-steps 1

# 5. å¯åŠ¨æœåŠ¡
echo "5ï¸âƒ£ Starting service..."
source config_production.env
./scripts/start_service_general.sh

echo "âœ… Quick start completed!"
echo "ğŸŒ API: http://localhost:8088"
echo "ğŸ“– Docs: http://localhost:8088/docs"
```

### éªŒè¯ API åŠŸèƒ½
```bash
# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 30

# å¥åº·æ£€æŸ¥
curl -s http://localhost:8088/health | jq '.'

# æµ‹è¯•è§†é¢‘ç”Ÿæˆ
curl -X POST http://localhost:8088/video/submit \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "A beautiful sunset over the ocean",
    "image_url": "https://picsum.photos/1280/720",
    "quality_preset": "balanced"
  }' | jq '.'

echo "ğŸš€ FastAPI Multi-GPU I2V is ready!"
echo "ğŸ’¡ Use tools/ and scripts/debug/ for monitoring and troubleshooting"
```

**ğŸŒŸ å¼€å§‹ä½ çš„ AI è§†é¢‘ç”Ÿæˆä¹‹æ—…ï¼**
