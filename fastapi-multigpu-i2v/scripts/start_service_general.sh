#!/bin/bash

echo "ğŸš€ Starting Universal Multi-GPU Video Generation API..."

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•å’Œé¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

echo "ğŸ“‚ Project root: $PROJECT_ROOT"
echo "ğŸ“‚ Script directory: $SCRIPT_DIR"

# è®¾ç½® Python è·¯å¾„ï¼ˆåœ¨è®¾å¤‡æ£€æµ‹ä¹‹å‰ï¼‰
export PYTHONPATH="$PROJECT_ROOT/src:$PROJECT_ROOT:${PYTHONPATH}"
echo "ğŸ PYTHONPATH: $PYTHONPATH"

# è‡ªåŠ¨æ£€æµ‹è®¾å¤‡ç±»å‹
echo "ğŸ” Detecting hardware devices..."
python3 -c "
import sys
import os

# ç¡®ä¿è·¯å¾„æ­£ç¡®
project_root = '$PROJECT_ROOT'
src_path = os.path.join(project_root, 'src')

# æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
if not os.path.exists(src_path):
    print(f'Error: src path does not exist: {src_path}', file=sys.stderr)
    print('DETECTED_DEVICE=cuda')
    print('DEVICE_COUNT=1')
    exit()

# æ·»åŠ åˆ° Python è·¯å¾„
if src_path not in sys.path:
    sys.path.insert(0, src_path)

print(f'DEBUG: Python paths added: {src_path}', file=sys.stderr)
print(f'DEBUG: Current sys.path: {sys.path[:3]}...', file=sys.stderr)

try:
    # å°è¯•å¯¼å…¥è®¾å¤‡æ£€æµ‹å™¨
    from utils.device_detector import device_detector
    device_type, device_count = device_detector.detect_device()
    print(f'DETECTED_DEVICE={device_type.value}')
    print(f'DEVICE_COUNT={device_count}')
    print(f'DEBUG: Successfully detected {device_type.value} with {device_count} devices', file=sys.stderr)
except ImportError as e:
    print(f'Import error: {e}', file=sys.stderr)
    # å°è¯•ç®€å•çš„è®¾å¤‡æ£€æµ‹
    try:
        import torch
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            print(f'DETECTED_DEVICE=cuda')
            print(f'DEVICE_COUNT={device_count}')
            print(f'DEBUG: Fallback CUDA detection: {device_count} devices', file=sys.stderr)
        else:
            print('DETECTED_DEVICE=cuda')
            print('DEVICE_COUNT=1')
            print('DEBUG: No CUDA available, using default', file=sys.stderr)
    except:
        print('DETECTED_DEVICE=cuda')
        print('DEVICE_COUNT=1')
        print('DEBUG: Fallback to default values', file=sys.stderr)
except Exception as e:
    print(f'Detection error: {e}', file=sys.stderr)
    print('DETECTED_DEVICE=cuda')
    print('DEVICE_COUNT=1')
" > device_info.tmp 2> device_debug.log

# æ£€æŸ¥è®¾å¤‡æ£€æµ‹æ˜¯å¦æˆåŠŸ
if [ ! -f device_info.tmp ] || [ ! -s device_info.tmp ]; then
    echo "âš ï¸  Device detection failed, using defaults..."
    echo "DETECTED_DEVICE=cuda" > device_info.tmp
    echo "DEVICE_COUNT=1" >> device_info.tmp
fi

# æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
if [ -f device_debug.log ]; then
    echo "ğŸ” Device detection debug info:"
    cat device_debug.log
    rm device_debug.log
fi

source device_info.tmp
rm device_info.tmp

echo "âœ… Detected device: $DETECTED_DEVICE with $DEVICE_COUNT devices"

# è®¾ç½®é€šç”¨ç¯å¢ƒå˜é‡
export TOKENIZERS_PARALLELISM=false
export MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}
export MASTER_PORT=${MASTER_PORT:-"29500"}

# æ ¹æ®è®¾å¤‡ç±»å‹è®¾ç½®ç‰¹å®šç¯å¢ƒå˜é‡
if [ "$DETECTED_DEVICE" = "npu" ]; then
    echo "âš™ï¸  Configuring for NPU (Ascend)..."
    export ALGO=0
    export PYTORCH_NPU_ALLOC_CONF='expandable_segments:True'
    export TASK_QUEUE_ENABLE=2
    export CPU_AFFINITY_CONF=1
    export HCCL_TIMEOUT=3600
    export HCCL_CONNECT_TIMEOUT=1200
    export HCCL_BUFFSIZE=256
    export ASCEND_LAUNCH_BLOCKING=0
    export ASCEND_GLOBAL_LOG_LEVEL=1
    
    # åŠ¨æ€è®¾ç½®å¯è§è®¾å¤‡
    if [ $DEVICE_COUNT -gt 0 ]; then
        DEVICE_LIST=$(seq -s, 0 $((DEVICE_COUNT-1)))
        export NPU_VISIBLE_DEVICES=$DEVICE_LIST
        export ASCEND_RT_VISIBLE_DEVICES=$DEVICE_LIST
        echo "ğŸ¯ NPU devices: $DEVICE_LIST"
    fi
    
elif [ "$DETECTED_DEVICE" = "cuda" ]; then
    echo "âš™ï¸  Configuring for CUDA (NVIDIA)..."
    export NCCL_TIMEOUT=3600
    export CUDA_LAUNCH_BLOCKING=0
    export NCCL_DEBUG=${NCCL_DEBUG:-"WARN"}
    export NCCL_IB_DISABLE=1
    export NCCL_P2P_DISABLE=0
    
    # åŠ¨æ€è®¾ç½®å¯è§è®¾å¤‡
    if [ $DEVICE_COUNT -gt 0 ]; then
        DEVICE_LIST=$(seq -s, 0 $((DEVICE_COUNT-1)))
        export CUDA_VISIBLE_DEVICES=$DEVICE_LIST
        echo "ğŸ¯ CUDA devices: $DEVICE_LIST"
    fi
    
else
    echo "âŒ Unsupported device: $DETECTED_DEVICE"
    echo "ğŸ’¡ Supported devices: npu, cuda"
    exit 1
fi

# æ¨¡å‹å’ŒæœåŠ¡é…ç½®
export MODEL_CKPT_DIR=${MODEL_CKPT_DIR:-"/data/models/modelscope/hub/Wan-AI/Wan2.1-I2V-14B-720P"}
export MODEL_TASK=${MODEL_TASK:-"i2v-14B"}
export DEFAULT_SIZE=${DEFAULT_SIZE:-"1280*720"}
export DEFAULT_FRAME_NUM=${DEFAULT_FRAME_NUM:-"81"}
export DEFAULT_SAMPLE_STEPS=${DEFAULT_SAMPLE_STEPS:-"30"}

# å¯é…ç½®çš„æ¨¡å‹é€‰é¡¹
export T5_CPU=${T5_CPU:-"false"}
export DIT_FSDP=${DIT_FSDP:-"true"}
export T5_FSDP=${T5_FSDP:-"false"}
export VAE_PARALLEL=${VAE_PARALLEL:-"true"}
export CFG_SIZE=${CFG_SIZE:-"1"}
export ULYSSES_SIZE=${ULYSSES_SIZE:-"8"}

# æ€§èƒ½ä¼˜åŒ–é…ç½®
export USE_ATTENTION_CACHE=${USE_ATTENTION_CACHE:-"true"}
export CACHE_START_STEP=${CACHE_START_STEP:-"12"}
export CACHE_INTERVAL=${CACHE_INTERVAL:-"4"}
export CACHE_END_STEP=${CACHE_END_STEP:-"37"}

# ä¸šåŠ¡é…ç½®ï¼ˆæ ¹æ®è®¾å¤‡å’Œ T5 æ¨¡å¼è‡ªåŠ¨è°ƒæ•´ï¼‰
if [ "$T5_CPU" = "true" ] || [ "$DETECTED_DEVICE" = "npu" ]; then
    export MAX_CONCURRENT_TASKS=${MAX_CONCURRENT_TASKS:-"2"}
    export TASK_TIMEOUT=${TASK_TIMEOUT:-"2400"}  # 40åˆ†é’Ÿ
    export OMP_NUM_THREADS=${OMP_NUM_THREADS:-"16"}  # T5 CPU éœ€è¦æ›´å¤šçº¿ç¨‹
    export MKL_NUM_THREADS=${MKL_NUM_THREADS:-"16"}
    export OPENBLAS_NUM_THREADS=${OPENBLAS_NUM_THREADS:-"16"}
else
    export MAX_CONCURRENT_TASKS=${MAX_CONCURRENT_TASKS:-"4"}
    export TASK_TIMEOUT=${TASK_TIMEOUT:-"1800"}  # 30åˆ†é’Ÿ
    export OMP_NUM_THREADS=${OMP_NUM_THREADS:-"8"}
    export MKL_NUM_THREADS=${MKL_NUM_THREADS:-"8"}
    export OPENBLAS_NUM_THREADS=${OPENBLAS_NUM_THREADS:-"8"}
fi

# æœåŠ¡é…ç½®
export SERVER_HOST=${SERVER_HOST:-"0.0.0.0"}
export SERVER_PORT=${SERVER_PORT:-"8088"}
export CLEANUP_INTERVAL=${CLEANUP_INTERVAL:-"300"}
export MAX_OUTPUT_DIR_SIZE=${MAX_OUTPUT_DIR_SIZE:-"50"}
export ALLOWED_HOSTS=${ALLOWED_HOSTS:-"*"}

echo "ğŸ“‹ Configuration Summary:"
echo "  - Device: $DETECTED_DEVICE ($DEVICE_COUNT cards)"
echo "  - Model Path: $MODEL_CKPT_DIR"
echo "  - T5 CPU Mode: $T5_CPU"
echo "  - DIT FSDP: $DIT_FSDP"
echo "  - VAE Parallel: $VAE_PARALLEL"
echo "  - Ulysses Size: $ULYSSES_SIZE"
echo "  - Attention Cache: $USE_ATTENTION_CACHE"
echo "  - Max Concurrent: $MAX_CONCURRENT_TASKS"
echo "  - Task Timeout: ${TASK_TIMEOUT}s"
echo "  - Server: $SERVER_HOST:$SERVER_PORT"
echo "  - CPU Threads: $OMP_NUM_THREADS"

# éªŒè¯é¡¹ç›®ç»“æ„
echo "ğŸ” Verifying project structure..."
if [ ! -f "$PROJECT_ROOT/src/utils/device_detector.py" ]; then
    echo "âš ï¸  device_detector.py not found at: $PROJECT_ROOT/src/utils/device_detector.py"
    echo "ğŸ’¡ Creating minimal device detector..."
    
    mkdir -p "$PROJECT_ROOT/src/utils"
    cat > "$PROJECT_ROOT/src/utils/__init__.py" << 'EOF'
# Utils package
EOF

    cat > "$PROJECT_ROOT/src/utils/device_detector.py" << 'EOF'
"""
è®¾å¤‡è‡ªåŠ¨æ£€æµ‹å·¥å…·
"""
from enum import Enum
from typing import Tuple

class DeviceType(Enum):
    NPU = "npu"
    CUDA = "cuda"

class DeviceDetector:
    """è®¾å¤‡æ£€æµ‹å™¨"""
    
    def detect_device(self) -> Tuple[DeviceType, int]:
        """æ£€æµ‹å¯ç”¨è®¾å¤‡"""
        # ä¼˜å…ˆæ£€æµ‹ NPU
        try:
            import torch_npu
            if torch_npu.npu.is_available():
                device_count = torch_npu.npu.device_count()
                return DeviceType.NPU, device_count
        except ImportError:
            pass
        
        # æ£€æµ‹ CUDA
        try:
            import torch
            if torch.cuda.is_available():
                device_count = torch.cuda.device_count()
                return DeviceType.CUDA, device_count
        except ImportError:
            pass
        
        # é»˜è®¤è¿”å›å•å¡ CUDA
        return DeviceType.CUDA, 1

# å…¨å±€å®ä¾‹
device_detector = DeviceDetector()
EOF
    echo "âœ… Created minimal device detector"
fi

# éªŒè¯æ¨¡å‹è·¯å¾„
if [ ! -d "$MODEL_CKPT_DIR" ]; then
    echo "âš ï¸  Model directory not found: $MODEL_CKPT_DIR"
    echo "ğŸ’¡ Please set MODEL_CKPT_DIR environment variable"
    echo "ğŸ’¡ Example: export MODEL_CKPT_DIR=/path/to/your/model"
fi

# æ¸…ç†æ—§è¿›ç¨‹
echo "ğŸ§¹ Cleaning up old processes..."
pkill -f "i2v_api.py" 2>/dev/null || true
pkill -f "torchrun.*i2v_api" 2>/dev/null || true
sleep 5

# åˆ›å»ºå¿…è¦ç›®å½•
echo "ğŸ“ Creating directories..."
mkdir -p "$PROJECT_ROOT/generated_videos"
mkdir -p "$PROJECT_ROOT/logs"

# è®¾ç½®å·¥ä½œç›®å½•
cd "$PROJECT_ROOT"
echo "ğŸ“ Working directory: $(pwd)"

# æ£€æŸ¥è®¾å¤‡çŠ¶æ€
echo "ğŸ” Checking device status..."
if [ "$DETECTED_DEVICE" = "npu" ]; then
    if command -v npu-smi &> /dev/null; then
        echo "NPU Status:"
        npu-smi info | head -10
    else
        echo "âš ï¸  npu-smi not available"
    fi
elif [ "$DETECTED_DEVICE" = "cuda" ]; then
    if command -v nvidia-smi &> /dev/null; then
        echo "GPU Status:"
        nvidia-smi --query-gpu=index,name,memory.total,memory.used,utilization.gpu --format=csv
    else
        echo "âš ï¸  nvidia-smi not available"
    fi
fi

# éªŒè¯ Python ç¯å¢ƒ
echo "ğŸ Checking Python environment..."
python3 -c "
import sys
import os
print(f'Python: {sys.version}')
print(f'Python executable: {sys.executable}')
print(f'Current working directory: {os.getcwd()}')
print(f'PYTHONPATH: {os.environ.get(\"PYTHONPATH\", \"Not set\")}')

# æ£€æŸ¥ src ç›®å½•
src_path = '$PROJECT_ROOT/src'
print(f'Checking src path: {src_path}')
print(f'Src path exists: {os.path.exists(src_path)}')

if os.path.exists(src_path):
    utils_path = os.path.join(src_path, 'utils')
    print(f'Utils path exists: {os.path.exists(utils_path)}')
    if os.path.exists(utils_path):
        detector_path = os.path.join(utils_path, 'device_detector.py')
        print(f'Device detector exists: {os.path.exists(detector_path)}')

try:
    if '$DETECTED_DEVICE' == 'npu':
        import torch_npu
        print(f'torch_npu available: {torch_npu.npu.is_available()}')
        if torch_npu.npu.is_available():
            print(f'NPU device count: {torch_npu.npu.device_count()}')
    elif '$DETECTED_DEVICE' == 'cuda':
        import torch
        print(f'PyTorch: {torch.__version__}')
        print(f'CUDA available: {torch.cuda.is_available()}')
        if torch.cuda.is_available():
            print(f'CUDA version: {torch.version.cuda}')
            print(f'GPU device count: {torch.cuda.device_count()}')
except ImportError as e:
    print(f'âŒ Import error: {e}')
    sys.exit(1)
except Exception as e:
    print(f'âš ï¸  Environment check warning: {e}')
"

if [ $? -ne 0 ]; then
    echo "âŒ Python environment check failed!"
    echo "ğŸ’¡ Please check your PyTorch installation"
    exit 1
fi

# æ¸…ç†è®¾å¤‡ç¼“å­˜
echo "ğŸ—‘ï¸  Clearing device cache..."
python3 -c "
try:
    if '$DETECTED_DEVICE' == 'npu':
        import torch_npu
        torch_npu.npu.empty_cache()
        print('âœ… NPU cache cleared')
    elif '$DETECTED_DEVICE' == 'cuda':
        import torch
        torch.cuda.empty_cache()
        print('âœ… CUDA cache cleared')
except Exception as e:
    print(f'âš ï¸  Cache clear warning: {e}')
"

# è®¾ç½®ä¿¡å·å¤„ç†
trap 'echo "ğŸ›‘ Stopping service..."; pkill -f "torchrun.*i2v_api"; exit 0' INT TERM

# å¯åŠ¨åˆ†å¸ƒå¼æœåŠ¡
echo ""
echo "ğŸš€ Starting $DEVICE_COUNT-card distributed service on $DETECTED_DEVICE..."
echo "ğŸ“¡ Master: $MASTER_ADDR:$MASTER_PORT"
echo "ğŸŒ Server will start on http://$SERVER_HOST:$SERVER_PORT"
echo "ğŸ“– API docs: http://$SERVER_HOST:$SERVER_PORT/docs"
echo ""

# å¯åŠ¨æœåŠ¡å¹¶è®°å½•æ—¥å¿—
LOG_FILE="$PROJECT_ROOT/logs/${DETECTED_DEVICE}_service_$(date +%Y%m%d_%H%M%S).log"

torchrun \
    --nproc_per_node=$DEVICE_COUNT \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    --nnodes=1 \
    --node_rank=0 \
    src/i2v_api.py 2>&1 | tee "$LOG_FILE"

echo "ğŸ Service stopped. Logs saved to: $LOG_FILE"