#!/bin/bash

echo "ğŸš€ Starting CUDA (NVIDIA) Multi-GPU Video Generation API..."

# è®¾ç½® CUDA ä¸“ç”¨ç¯å¢ƒå˜é‡
export NCCL_TIMEOUT=${NCCL_TIMEOUT:-"3600"}
export CUDA_LAUNCH_BLOCKING=${CUDA_LAUNCH_BLOCKING:-"0"}
export NCCL_DEBUG=${NCCL_DEBUG:-"INFO"}
export NCCL_IB_DISABLE=${NCCL_IB_DISABLE:-"1"}
export NCCL_P2P_DISABLE=${NCCL_P2P_DISABLE:-"0"}

# GPU è®¾å¤‡é…ç½®
export DEVICE_COUNT=${DEVICE_COUNT:-$(nvidia-smi -L | wc -l || echo "8")}
export CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-"0,1,2,3,4,5,6,7"}

# é€šç”¨åˆ†å¸ƒå¼é…ç½®
export TOKENIZERS_PARALLELISM=false
export MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}
export MASTER_PORT=${MASTER_PORT:-"29500"}

# æ¨¡å‹é…ç½® (CUDA ä¼˜åŒ–)
export T5_CPU=${T5_CPU:-"false"}          # CUDA å¯ä»¥ T5 ç”¨ GPU
export DIT_FSDP=${DIT_FSDP:-"true"}
export T5_FSDP=${T5_FSDP:-"false"}
export VAE_PARALLEL=${VAE_PARALLEL:-"true"}
export CFG_SIZE=${CFG_SIZE:-"1"}
export ULYSSES_SIZE=${ULYSSES_SIZE:-"8"}

# æ€§èƒ½ä¼˜åŒ–é…ç½®
export USE_ATTENTION_CACHE=${USE_ATTENTION_CACHE:-"true"}
export CACHE_START_STEP=${CACHE_START_STEP:-"12"}
export CACHE_INTERVAL=${CACHE_INTERVAL:-"4"}
export CACHE_END_STEP=${CACHE_END_STEP:-"37"}

# ä¸šåŠ¡é…ç½® (CUDA æ›´é«˜æ€§èƒ½)
export MAX_CONCURRENT_TASKS=${MAX_CONCURRENT_TASKS:-"4"}
export TASK_TIMEOUT=${TASK_TIMEOUT:-"1800"}        # 30åˆ†é’Ÿï¼ŒCUDA è¾ƒå¿«
export CLEANUP_INTERVAL=${CLEANUP_INTERVAL:-"300"}
export MAX_OUTPUT_DIR_SIZE=${MAX_OUTPUT_DIR_SIZE:-"50"}
export SERVER_PORT=${SERVER_PORT:-"8088"}
export ALLOWED_HOSTS=${ALLOWED_HOSTS:-"*"}

# æ¨¡å‹è·¯å¾„
export MODEL_CKPT_DIR=${MODEL_CKPT_DIR:-"/data/models/modelscope/hub/Wan-AI/Wan2.1-I2V-14B-720P"}
export MODEL_TASK=${MODEL_TASK:-"i2v-14B"}
export DEFAULT_SIZE=${DEFAULT_SIZE:-"1280*720"}
export DEFAULT_FRAME_NUM=${DEFAULT_FRAME_NUM:-"81"}
export DEFAULT_SAMPLE_STEPS=${DEFAULT_SAMPLE_STEPS:-"40"}

# CPU çº¿ç¨‹ä¼˜åŒ– (CUDA å¯ä»¥ç”¨æ›´å°‘ CPU)
export OMP_NUM_THREADS=${OMP_NUM_THREADS:-"8"}
export MKL_NUM_THREADS=${MKL_NUM_THREADS:-"8"}
export OPENBLAS_NUM_THREADS=${OPENBLAS_NUM_THREADS:-"8"}

# Python è·¯å¾„
export PYTHONPATH=/workspace/Wan2.1:$PYTHONPATH

echo "ğŸ“‹ CUDA Configuration:"
echo "  - Device Count: $DEVICE_COUNT"
echo "  - CUDA Devices: $CUDA_VISIBLE_DEVICES"
echo "  - T5 CPU Mode: $T5_CPU"
echo "  - DIT FSDP: $DIT_FSDP"
echo "  - VAE Parallel: $VAE_PARALLEL"
echo "  - Max Concurrent: $MAX_CONCURRENT_TASKS"
echo "  - Timeout: ${TASK_TIMEOUT}s"
echo "  - Server Port: $SERVER_PORT"
echo "  - Model Path: $MODEL_CKPT_DIR"

# æ£€æŸ¥ CUDA ç¯å¢ƒ
echo "ğŸ” Checking CUDA environment..."
if command -v nvidia-smi &> /dev/null; then
    echo "GPU Status:"
    nvidia-smi --query-gpu=index,name,memory.total,memory.used,utilization.gpu --format=csv
else
    echo "âš ï¸  nvidia-smi not found, continuing anyway..."
fi

# æ£€æŸ¥ Python ç¯å¢ƒ
echo "ğŸ Checking Python environment..."
python3 -c "
import sys
print(f'Python: {sys.version}')
try:
    import torch
    print(f'PyTorch: {torch.__version__}')
    print(f'CUDA available: {torch.cuda.is_available()}')
    if torch.cuda.is_available():
        print(f'CUDA version: {torch.version.cuda}')
        print(f'GPU count: {torch.cuda.device_count()}')
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f'  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)')
except ImportError as e:
    print(f'âŒ torch import failed: {e}')
    sys.exit(1)
"

if [ $? -ne 0 ]; then
    echo "âŒ Python environment check failed!"
    exit 1
fi

# æ¸…ç†æ—§è¿›ç¨‹
echo "ğŸ§¹ Cleaning up old processes..."
pkill -f "i2v_api.py" || true
pkill -f "torchrun.*i2v_api" || true
sleep 3

# åˆ›å»ºå¿…è¦ç›®å½•
mkdir -p generated_videos
mkdir -p logs

# æ¸…ç† CUDA ç¼“å­˜
echo "ğŸ—‘ï¸  Clearing CUDA cache..."
python3 -c "
try:
    import torch
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print('âœ… CUDA cache cleared')
    else:
        print('âš ï¸  CUDA not available')
except Exception as e:
    print(f'âš ï¸  Cache clear warning: {e}')
"

# è®¾ç½®ä¿¡å·å¤„ç†
trap 'echo "ğŸ›‘ Stopping CUDA service..."; pkill -f "torchrun.*i2v_api"; exit 0' INT TERM

# å¯åŠ¨ CUDA åˆ†å¸ƒå¼æœåŠ¡
echo "ğŸš€ Starting $DEVICE_COUNT-GPU distributed service..."
echo "ğŸ“¡ Master: $MASTER_ADDR:$MASTER_PORT"
echo "ğŸŒ Server will start on port $SERVER_PORT"
echo ""

# ä½¿ç”¨æ›´é«˜æ€§èƒ½çš„ NCCL å¯åŠ¨å‚æ•°
torchrun \
    --nproc_per_node=$DEVICE_COUNT \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    --nnodes=1 \
    --node_rank=0 \
    src/i2v_api.py 2>&1 | tee logs/cuda_service.log

echo "ğŸ CUDA service stopped."