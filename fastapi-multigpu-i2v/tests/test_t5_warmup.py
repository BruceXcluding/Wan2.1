#!/usr/bin/env python3
# filepath: /Users/yigex/Documents/LLM-Inftra/Wan2.1_fix/fastapi-multigpu-i2v/tests/test_warmup.py
"""
T5 é¢„çƒ­æµ‹è¯•å·¥å…·
æ•´åˆäº†åŸ debug/debug_t5_warmup.py çš„åŠŸèƒ½
"""

import sys
import os
import time
import logging
import argparse
from pathlib import Path

# è®¾ç½®é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
os.environ.setdefault("T5_CPU", "true")

# é…ç½®æ—¥å¿—
logs_dir = project_root / "logs"
logs_dir.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(logs_dir / "t5_warmup_test.log", encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class T5WarmupTester:
    """T5 é¢„çƒ­æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.test_results = {}
    
    def setup_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ”§ Setting up environment...")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env_vars = {
            "T5_CPU": "true",
            "DIT_FSDP": "false",
            "T5_FSDP": "false",
            "VAE_PARALLEL": "false",
            "RANK": "0",
            "WORLD_SIZE": "1",
            "LOCAL_RANK": "0"
        }
        
        for key, value in env_vars.items():
            if key not in os.environ:
                os.environ[key] = value
                logger.info(f"Set {key}={value}")
        
        logger.info("âœ… Environment setup completed")
    
    def check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
        logger.info("ğŸ’» Checking system resources...")
        
        try:
            import psutil
            
            # æ£€æŸ¥å†…å­˜
            memory = psutil.virtual_memory()
            logger.info(f"System memory: {memory.total / 1024**3:.2f}GB total, "
                       f"{memory.available / 1024**3:.2f}GB available")
            
            if memory.available < 4 * 1024**3:  # 4GB
                logger.warning("Available memory is low (< 4GB)")
            
            # æ£€æŸ¥ CPU
            cpu_count = psutil.cpu_count()
            logger.info(f"CPU cores: {cpu_count}")
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´
            disk = psutil.disk_usage('/')
            logger.info(f"Disk space: {disk.free / 1024**3:.2f}GB free")
            
            return True
            
        except ImportError:
            logger.warning("psutil not available, skipping resource check")
            return True
        except Exception as e:
            logger.error(f"System resource check failed: {e}")
            return False
    
    def test_imports(self):
        """æµ‹è¯•å¿…è¦çš„å¯¼å…¥"""
        logger.info("ğŸ“¦ Testing imports...")
        
        imports = [
            ("torch", "PyTorch"),
            ("wan", "WAN model"),
            ("wan.configs", "WAN configs"),
        ]
        
        failed = 0
        
        for module_name, description in imports:
            try:
                __import__(module_name)
                logger.info(f"âœ… {description} imported successfully")
            except ImportError as e:
                logger.error(f"âŒ Failed to import {description}: {e}")
                failed += 1
            except Exception as e:
                logger.error(f"âŒ Error importing {description}: {e}")
                failed += 1
        
        if failed == 0:
            logger.info("âœ… All imports successful")
            return True
        else:
            logger.error(f"âŒ {failed} imports failed")
            return False
    
    def test_t5_cpu_warmup(self, model_path: str = None, max_warmup_steps: int = 3):
        """æµ‹è¯• T5 CPU é¢„çƒ­"""
        logger.info("ğŸ”¥ Starting T5 CPU warmup test...")
        
        if not model_path:
            model_path = os.environ.get('MODEL_CKPT_DIR', 
                "/data/models/modelscope/hub/Wan-AI/Wan2.1-I2V-14B-720P")
        
        logger.info(f"Model path: {model_path}")
        
        try:
            # å¯¼å…¥å¿…è¦æ¨¡å—
            import torch
            import wan
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„
            if not os.path.exists(model_path):
                logger.warning(f"Model path does not exist: {model_path}")
                logger.info("Continuing with test (model will be downloaded if needed)")
            
            # è·å–é…ç½®
            logger.info("Loading model configuration...")
            cfg = wan.configs.WAN_CONFIGS["i2v-14B"]
            logger.info(f"Config loaded: {type(cfg).__name__}")
            
            # è®°å½•åˆå§‹å†…å­˜
            try:
                if torch.cuda.is_available():
                    initial_gpu_memory = torch.cuda.memory_allocated(0) / 1024**3
                    logger.info(f"Initial GPU memory: {initial_gpu_memory:.2f}GB")
                elif hasattr(torch, 'npu') or 'torch_npu' in sys.modules:
                    try:
                        import torch_npu
                        initial_npu_memory = torch_npu.npu.memory_allocated(0) / 1024**3
                        logger.info(f"Initial NPU memory: {initial_npu_memory:.2f}GB")
                    except:
                        pass
            except:
                pass
            
            # åŠ è½½æ¨¡å‹ - ç®€åŒ–ç‰ˆæœ¬ç”¨äºæµ‹è¯•
            logger.info("Creating simplified model instance for T5 test...")
            load_start = time.time()
            
            # åˆ›å»ºä¸€ä¸ªæœ€å°é…ç½®çš„æ¨¡å‹å®ä¾‹æ¥æµ‹è¯• T5
            try:
                model = wan.WanI2V(
                    config=cfg,
                    checkpoint_dir=model_path,
                    device_id=0,
                    rank=0,
                    t5_cpu=True,      # å¼ºåˆ¶ T5 ä½¿ç”¨ CPU
                    dit_fsdp=False,   # å…³é—­ FSDP
                    t5_fsdp=False,    # å…³é—­ T5 FSDP
                    use_vae_parallel=False,  # å…³é—­ VAE å¹¶è¡Œ
                    use_usp=False     # å…³é—­ USP
                )
                
                load_time = time.time() - load_start
                logger.info(f"Model instance created in {load_time:.2f}s")
                
            except Exception as e:
                logger.error(f"Failed to create full model, trying T5 only: {e}")
                # å¦‚æœå®Œæ•´æ¨¡å‹åˆ›å»ºå¤±è´¥ï¼Œå°è¯•åªæµ‹è¯• T5 ç¼–ç å™¨
                logger.info("Attempting to test T5 encoder directly...")
                return self._test_t5_encoder_only(max_warmup_steps)
            
            # è®°å½•åŠ è½½åçš„å†…å­˜ä½¿ç”¨
            try:
                if torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated(0) / 1024**3
                    reserved = torch.cuda.memory_reserved(0) / 1024**3
                    logger.info(f"GPU memory after model load: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB")
                elif hasattr(torch, 'npu') or 'torch_npu' in sys.modules:
                    try:
                        import torch_npu
                        allocated = torch_npu.npu.memory_allocated(0) / 1024**3
                        reserved = torch_npu.npu.memory_reserved(0) / 1024**3
                        logger.info(f"NPU memory after model load: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB")
                    except:
                        pass
            except:
                pass
            
            # T5 é¢„çƒ­æµ‹è¯•
            logger.info(f"Starting T5 warmup with {max_warmup_steps} steps...")
            
            dummy_prompts = [
                "warm up text for testing",
                "a beautiful landscape with mountains",
                "testing T5 encoder performance",
                "video generation prompt",
                "final warmup step test"
            ][:max_warmup_steps]
            
            warmup_start = time.time()
            successful_steps = 0
            
            with torch.no_grad():
                for i, prompt in enumerate(dummy_prompts):
                    logger.info(f"Warmup step {i+1}/{len(dummy_prompts)}: '{prompt}'")
                    step_start = time.time()
                    
                    try:
                        # è°ƒç”¨ T5 ç¼–ç å™¨
                        if hasattr(model, 'text_encoder') and callable(model.text_encoder):
                            result = model.text_encoder([prompt], torch.device('cpu'))
                        else:
                            logger.warning("T5 encoder not accessible, skipping step")
                            continue
                            
                        step_time = time.time() - step_start
                        
                        # è®°å½•ç»“æœä¿¡æ¯
                        if hasattr(result, 'shape'):
                            logger.info(f"Step {i+1} SUCCESS in {step_time:.2f}s, result shape: {result.shape}")
                        elif isinstance(result, (list, tuple)) and len(result) > 0:
                            logger.info(f"Step {i+1} SUCCESS in {step_time:.2f}s, result length: {len(result)}")
                        else:
                            logger.info(f"Step {i+1} SUCCESS in {step_time:.2f}s, result type: {type(result)}")
                        
                        successful_steps += 1
                        
                    except Exception as e:
                        logger.error(f"Step {i+1} FAILED: {str(e)}")
                        # ç»§ç»­å…¶ä»–æ­¥éª¤
                        continue
            
            total_warmup_time = time.time() - warmup_start
            
            # è¾“å‡ºæ€»ç»“
            logger.info("=" * 60)
            logger.info("T5 WARMUP SUMMARY")
            logger.info("=" * 60)
            logger.info(f"Total warmup time: {total_warmup_time:.2f}s")
            logger.info(f"Successful steps: {successful_steps}/{len(dummy_prompts)}")
            if len(dummy_prompts) > 0:
                logger.info(f"Average time per step: {total_warmup_time/len(dummy_prompts):.2f}s")
            
            # æ¸…ç†
            try:
                if hasattr(model, 'cleanup'):
                    model.cleanup()
                del model
            except:
                pass
            
            self.test_results['t5_warmup'] = {
                'passed': successful_steps > 0,
                'successful_steps': successful_steps,
                'total_steps': len(dummy_prompts),
                'total_time': total_warmup_time,
                'avg_time_per_step': total_warmup_time/len(dummy_prompts) if len(dummy_prompts) > 0 else 0
            }
            
            if successful_steps > 0:
                logger.info("âœ… T5 CPU warmup test PASSED")
                return True
            else:
                logger.error("âŒ T5 CPU warmup test FAILED")
                return False
                
        except Exception as e:
            logger.error(f"T5 CPU warmup test failed: {str(e)}")
            import traceback
            logger.error(f"Full traceback:\n{traceback.format_exc()}")
            return False
    
    def _test_t5_encoder_only(self, max_warmup_steps: int):
        """åªæµ‹è¯• T5 ç¼–ç å™¨"""
        logger.info("ğŸ”¥ Testing T5 encoder only...")
        
        try:
            import torch
            from transformers import T5EncoderModel, T5Tokenizer
            
            logger.info("Loading T5 encoder and tokenizer...")
            
            # ä½¿ç”¨è¾ƒå°çš„ T5 æ¨¡å‹è¿›è¡Œæµ‹è¯•
            model_name = "t5-base"  # å¯ä»¥æ”¹ä¸ºé¡¹ç›®ä¸­ä½¿ç”¨çš„å…·ä½“ T5 æ¨¡å‹
            
            tokenizer = T5Tokenizer.from_pretrained(model_name)
            encoder = T5EncoderModel.from_pretrained(model_name)
            
            # ç¡®ä¿åœ¨ CPU ä¸Š
            encoder = encoder.to('cpu')
            encoder.eval()
            
            logger.info("T5 encoder loaded successfully")
            
            dummy_prompts = [
                "warm up text for testing",
                "a beautiful landscape with mountains",
                "testing T5 encoder performance"
            ][:max_warmup_steps]
            
            successful_steps = 0
            warmup_start = time.time()
            
            with torch.no_grad():
                for i, prompt in enumerate(dummy_prompts):
                    logger.info(f"T5 warmup step {i+1}/{len(dummy_prompts)}: '{prompt}'")
                    step_start = time.time()
                    
                    try:
                        # ç¼–ç æ–‡æœ¬
                        inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)
                        outputs = encoder(**inputs)
                        
                        step_time = time.time() - step_start
                        logger.info(f"Step {i+1} SUCCESS in {step_time:.2f}s, output shape: {outputs.last_hidden_state.shape}")
                        successful_steps += 1
                        
                    except Exception as e:
                        logger.error(f"Step {i+1} FAILED: {str(e)}")
                        continue
            
            total_time = time.time() - warmup_start
            logger.info(f"T5 encoder test completed: {successful_steps}/{len(dummy_prompts)} successful")
            
            return successful_steps > 0
            
        except Exception as e:
            logger.error(f"T5 encoder only test failed: {e}")
            return False
    
    def run_all_tests(self, model_path: str = None, warmup_steps: int = 3, 
                     skip_resource_check: bool = False, skip_import_test: bool = False):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ T5 Warmup Test Suite")
        logger.info("=" * 60)
        
        # è®¾ç½®ç¯å¢ƒ
        self.setup_environment()
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        if not skip_resource_check:
            if not self.check_system_resources():
                logger.warning("System resource check failed, continuing anyway...")
        
        # æµ‹è¯•å¯¼å…¥
        if not skip_import_test:
            if not self.test_imports():
                logger.error("Import test failed, aborting")
                return False
        
        # æµ‹è¯• T5 é¢„çƒ­
        success = self.test_t5_cpu_warmup(model_path, warmup_steps)
        
        if success:
            logger.info("ğŸ‰ All T5 warmup tests PASSED!")
        else:
            logger.error("ğŸ’¥ T5 warmup tests FAILED!")
        
        return success

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="T5 Warmup Test Tool")
    parser.add_argument("--model-path", type=str, 
                       help="Path to model checkpoint directory")
    parser.add_argument("--warmup-steps", type=int, default=3,
                       help="Number of warmup steps to test")
    parser.add_argument("--skip-resource-check", action="store_true",
                       help="Skip system resource check")
    parser.add_argument("--skip-import-test", action="store_true",
                       help="Skip import test")
    
    args = parser.parse_args()
    
    logger.info("ğŸš€ T5 Warmup Test Tool")
    logger.info("=" * 60)
    logger.info(f"Model path: {args.model_path or 'default'}")
    logger.info(f"Warmup steps: {args.warmup_steps}")
    logger.info("=" * 60)
    
    try:
        tester = T5WarmupTester()
        success = tester.run_all_tests(
            model_path=args.model_path,
            warmup_steps=args.warmup_steps,
            skip_resource_check=args.skip_resource_check,
            skip_import_test=args.skip_import_test
        )
        
        return 0 if success else 1
        
    except KeyboardInterrupt:
        logger.info("â¸ï¸  Test interrupted by user")
        return 130
    except Exception as e:
        logger.error(f"ğŸ’¥ Test suite failed: {e}")
        import traceback
        logger.error(f"Full traceback:\n{traceback.format_exc()}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)