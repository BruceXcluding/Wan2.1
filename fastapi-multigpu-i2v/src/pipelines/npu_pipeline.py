"""
ç®€åŒ–çš„NPUç®¡é“å®ç°
"""
import os
import torch
from typing import Optional
from .base_pipeline import BasePipeline

# NPU ç‰¹å®šå¯¼å…¥
try:
    import torch_npu
    from torch_npu.contrib import transfer_to_npu
    NPU_AVAILABLE = True
except ImportError:
    NPU_AVAILABLE = False

import logging
logger = logging.getLogger(__name__)

class NPUPipeline(BasePipeline):
    """åä¸ºæ˜‡è…¾ NPU è§†é¢‘ç”Ÿæˆç®¡é“ - ç®€åŒ–ç‰ˆ"""
    
    def __init__(self, ckpt_dir: str, **model_args):
        if not NPU_AVAILABLE:
            raise RuntimeError("torch_npu not available, cannot use NPU pipeline")
        
        self.device_type = "npu"
        super().__init__(ckpt_dir, **model_args)
        
        # åˆå§‹åŒ–
        self.initialize()
    
    def _setup_environment(self):
        """è®¾ç½®NPUç¯å¢ƒå˜é‡"""
        os.environ.setdefault("ASCEND_LAUNCH_BLOCKING", "0")
        os.environ.setdefault("HCCL_TIMEOUT", "1800")
        os.environ.setdefault("HCCL_BUFFSIZE", "512")
        os.environ.setdefault("HCCL_CONNECT_TIMEOUT", "600")
        
        # NPU ç‰¹å®šé…ç½®
        torch_npu.npu.set_compile_mode(jit_compile=False)
        torch.npu.config.allow_internal_format = False
    
    def _set_device(self):
        """è®¾ç½®NPUè®¾å¤‡"""
        torch.npu.set_device(self.local_rank)
    
    def _move_to_device(self, tensor):
        """ç§»åŠ¨å¼ é‡åˆ°NPU"""
        return tensor.npu()
    
    def _configure_model(self, model):
        """é…ç½®NPUæ¨¡å‹"""
        # NPUç‰¹å®šçš„æ¨¡å‹é…ç½®å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
        pass
    

    def _generate_video_device_specific(self, request, img, size: str, frame_num: int):
        """NPUç‰¹å®šçš„è§†é¢‘ç”Ÿæˆé€»è¾‘"""
        try:
            logger.info(f"NPU generating video: size={size}, frames={frame_num}")
            
            # ğŸ”§ å‚æ•°æ˜ å°„ - ä¸¥æ ¼æŒ‰ç…§ generate.py:428-437
            generation_params = {
                'max_area': self._calculate_max_area(size),      # æ ¹æ®å°ºå¯¸è®¡ç®—
                'frame_num': frame_num,                          # å¸§æ•°
                'shift': 3.0,                                    # i2v é»˜è®¤ shift
                'sample_solver': 'unipc',                        # é»˜è®¤é‡‡æ ·å™¨
                'sampling_steps': request.infer_steps or 40,     # æ¨ç†æ­¥æ•°
                'guide_scale': request.guidance_scale or 5.0,    # å¼•å¯¼ç³»æ•°
                'seed': request.seed or self._generate_seed(),   # éšæœºç§å­
                'offload_model': False                           # NPU é€šå¸¸ä¸éœ€è¦å¸è½½
            }
            
            logger.info(f"NPU generation params: {generation_params}")
            
            # è°ƒç”¨æ¨¡å‹ç”Ÿæˆ - ä¸ generate.py:428-437 å®Œå…¨ä¸€è‡´
            video_tensor = self.model.generate(
                request.prompt,  # ç¬¬ä¸€ä¸ªå‚æ•°ï¼šæç¤ºè¯
                img,            # ç¬¬äºŒä¸ªå‚æ•°ï¼šPIL.Image å¯¹è±¡
                **generation_params  # å…¶ä½™å‚æ•°
            )
            
            if self.rank == 0:
                return video_tensor
            else:
                return None
                
        except Exception as e:
            logger.error(f"NPU video generation failed: {str(e)}")
            raise
        
def _calculate_max_area(self, size: str) -> int:
    """æ ¹æ®å°ºå¯¸å­—ç¬¦ä¸²è®¡ç®— max_area"""
    try:
        if '*' in size:
            width, height = map(int, size.split('*'))
            return width * height
        else:
            # é»˜è®¤å€¼
            return 921600  # 1280*720
    except:
        return 921600
        
def _generate_seed(self) -> int:
    """ç”Ÿæˆéšæœºç§å­"""
    import random
    return random.randint(0, 2**32 - 1)
    
    def _log_memory_usage(self):
        """è®°å½•NPUå†…å­˜ä½¿ç”¨"""
        try:
            memory_allocated = torch.npu.memory_allocated(self.local_rank) / 1024**3
            memory_reserved = torch.npu.memory_reserved(self.local_rank) / 1024**3
            logger.info(f"Rank {self.rank} NPU Memory - Allocated: {memory_allocated:.2f}GB, Reserved: {memory_reserved:.2f}GB")
        except Exception as e:
            logger.warning(f"Failed to get NPU memory info: {str(e)}")
    
    def _empty_cache(self):
        """æ¸…ç†NPUç¼“å­˜"""
        torch.npu.empty_cache()
    
    def _get_backend(self) -> str:
        """è·å–NPUåˆ†å¸ƒå¼åç«¯"""
        return "hccl"