"""
é‡æ„åçš„åŸºç¡€ç®¡é“ç±» - ä½¿ç”¨æ¨¡æ¿æ–¹æ³•æ¨¡å¼å’Œæ··å…¥ç±»
"""
import abc
import os
import time
import logging
import torch
import torch.distributed as dist
from datetime import timedelta
from typing import Optional, Callable, Dict, Any
from pathlib import Path
from PIL import Image
import requests
from io import BytesIO

logger = logging.getLogger(__name__)

class DistributedMixin:
    """åˆ†å¸ƒå¼åŠŸèƒ½æ··å…¥ç±»"""
    
    def _init_distributed(self):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
        try:
            self.world_size = int(os.environ.get("WORLD_SIZE", 1))
            self.rank = int(os.environ.get("RANK", 0))
            self.local_rank = int(os.environ.get("LOCAL_RANK", 0))
            
            if self.world_size > 1:
                logger.info(f"Initializing distributed: rank={self.rank}, world_size={self.world_size}, local_rank={self.local_rank}")
                
                # ğŸ”§ è®¾å¤‡ç‰¹å®šçš„åˆ†å¸ƒå¼åˆå§‹åŒ–
                if self.device_type == "npu":
                    # NPU åˆ†å¸ƒå¼åˆå§‹åŒ–
                    import torch_npu
                    
                    # è®¾ç½®å½“å‰è®¾å¤‡
                    torch_npu.npu.set_device(self.local_rank)
                    
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ HCCL åç«¯å¹¶å¢åŠ è¶…æ—¶
                    dist.init_process_group(
                        backend="hccl",  # NPU ä½¿ç”¨ HCCL åç«¯
                        init_method=f"env://",
                        world_size=self.world_size,
                        rank=self.rank,
                        timeout=timedelta(seconds=3600)  # å¢åŠ è¶…æ—¶æ—¶é—´
                    )
                    
                    logger.info(f"âœ… NPU distributed initialized: rank={self.rank}")
                    
                elif self.device_type == "cuda":
                    # CUDA åˆ†å¸ƒå¼åˆå§‹åŒ–
                    torch.cuda.set_device(self.local_rank)
                    
                    dist.init_process_group(
                        backend="nccl",
                        init_method="env://",
                        world_size=self.world_size,
                        rank=self.rank,
                        timeout=timedelta(seconds=1800)
                    )
                    
                    logger.info(f"âœ… CUDA distributed initialized: rank={self.rank}")
                    
                else:
                    # CPU åˆ†å¸ƒå¼åˆå§‹åŒ–
                    dist.init_process_group(
                        backend="gloo",
                        init_method="env://", 
                        world_size=self.world_size,
                        rank=self.rank,
                        timeout=timedelta(seconds=1800)
                    )
                    
                    logger.info(f"âœ… CPU distributed initialized: rank={self.rank}")
            else:
                logger.info("Single device mode, skipping distributed initialization")
                
        except Exception as e:
            logger.error(f"Distributed initialization failed: {e}")
            # ğŸ”§ é‡è¦ï¼šä¸è¦ raiseï¼Œç»§ç»­ä»¥å•è®¾å¤‡æ¨¡å¼è¿è¡Œ
            self.world_size = 1
            self.rank = 0
            self.local_rank = 0
            logger.warning("Falling back to single device mode")
        
    def _test_communication(self):
        """æµ‹è¯•åˆ†å¸ƒå¼é€šä¿¡"""
        logger.info(f"Testing {self.device_type} distributed communication...")
        test_tensor = torch.tensor([self.rank], dtype=torch.float32)
        test_tensor = self._move_to_device(test_tensor)
        dist.all_reduce(test_tensor)
        logger.info(f"{self.device_type} communication test passed, sum: {test_tensor.item()}")
    
    def _broadcast_image_path_common(self, image_path: Optional[str]) -> str:
        """é€šç”¨å›¾ç‰‡è·¯å¾„å¹¿æ’­é€»è¾‘"""
        if self.world_size == 1:
            return image_path
        
        try:
            # åˆ›å»ºè·¯å¾„å­—ç¬¦ä¸²çš„å¼ é‡
            if self.rank == 0:
                path_str = image_path or ""
                path_bytes = path_str.encode('utf-8')
                size_tensor = torch.tensor([len(path_bytes)], dtype=torch.long)
                path_tensor = torch.tensor(list(path_bytes), dtype=torch.uint8)
            else:
                size_tensor = torch.tensor([0], dtype=torch.long)
                path_tensor = None
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            size_tensor = self._move_to_device(size_tensor)
            
            # å¹¿æ’­å¤§å°
            dist.broadcast(size_tensor, 0)
            
            # åˆ›å»ºæ¥æ”¶å¼ é‡
            if self.rank != 0:
                path_tensor = torch.zeros(size_tensor[0].item(), dtype=torch.uint8)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶å¹¿æ’­è·¯å¾„
            if size_tensor[0].item() > 0:
                path_tensor = self._move_to_device(path_tensor)
                dist.broadcast(path_tensor, 0)
            
            # è½¬æ¢å›å­—ç¬¦ä¸²
            if size_tensor[0].item() > 0:
                received_path = bytes(path_tensor.cpu().tolist()).decode('utf-8')
            else:
                received_path = ""
            
            logger.info(f"Rank {self.rank} received image path: {received_path}")
            return received_path
            
        except Exception as e:
            logger.error(f"Image path broadcast failed: {e}")
            raise

class ModelLoaderMixin:
    """æ¨¡å‹åŠ è½½åŠŸèƒ½æ··å…¥ç±»"""
    def _load_model_common(self, model_config: Dict[str, Any]):
        """é€šç”¨æ¨¡å‹åŠ è½½é€»è¾‘"""
        try:
            # ç¡®ä¿èƒ½æ‰¾åˆ° wan æ¨¡å—
            import sys
            from pathlib import Path

            # æ·»åŠ  wan æ¨¡å—è·¯å¾„
            current_file = Path(__file__).resolve()
            project_root = current_file.parent.parent.parent
            wan_project_root = project_root.parent

            if str(wan_project_root) not in sys.path:
                sys.path.insert(0, str(wan_project_root))
                logger.info(f"Added wan project path: {wan_project_root}")

            # å¯¼å…¥ wan æ¨¡å—
            import wan
            logger.info("âœ… Successfully imported wan module")

            # è·å–é…ç½®
            cfg = wan.configs.WAN_CONFIGS[model_config.get("task", "i2v-14B")]

            # è·å–åˆ†å¸ƒå¼å‚æ•°
            ulysses_size = model_config.get('ulysses_size', 1)
            ring_size = model_config.get('ring_size', 1)
            use_usp = ulysses_size > 1 or ring_size > 1

            logger.info(f"Distributed params: ulysses_size={ulysses_size}, ring_size={ring_size}, use_usp={use_usp}")

            # åˆ†å¸ƒå¼ç¯å¢ƒéªŒè¯å’Œåˆå§‹åŒ– (å‚ç…§ generate.py:333-346)
            world_size = getattr(self, 'world_size', 1)
            rank = getattr(self, 'rank', 0)

            if world_size > 1:
                if use_usp:
                    # éªŒè¯å‚æ•° (generate.py:334)
                    assert ulysses_size * ring_size == world_size, \
                        f"ulysses_size({ulysses_size}) * ring_size({ring_size}) != world_size({world_size})"

                    # éªŒè¯æ³¨æ„åŠ›å¤´æ•°å…¼å®¹æ€§ (generate.py:359)
                    if ulysses_size > 1:
                        assert cfg.num_heads % ulysses_size == 0, \
                            f"`{cfg.num_heads=}` cannot be divided evenly by `{ulysses_size=}`."

                    try:
                        # åˆå§‹åŒ– xfuser åˆ†å¸ƒå¼ç¯å¢ƒ (generate.py:335-346)
                        import torch.distributed as dist
                        from xfuser.core.distributed import (
                            init_distributed_environment,
                            initialize_model_parallel,
                        )

                        if dist.is_initialized():
                            logger.info(f"Initializing xfuser: ulysses_size={ulysses_size}, ring_size={ring_size}")

                            init_distributed_environment(
                                rank=dist.get_rank(), 
                                world_size=dist.get_world_size()
                            )

                            initialize_model_parallel(
                                sequence_parallel_degree=dist.get_world_size(),
                                ring_degree=ring_size,
                                ulysses_degree=ulysses_size,
                            )

                            logger.info("âœ… xfuser initialized successfully")
                        else:
                            logger.warning("PyTorch distributed not initialized, disabling USP")
                            use_usp = False

                    except ImportError as e:
                        logger.warning(f"xfuser not available: {e}, disabling USP")
                        use_usp = False
                    except Exception as e:
                        logger.warning(f"xfuser initialization failed: {e}, disabling USP") 
                        use_usp = False
            else:
                # å•è¿›ç¨‹ç¯å¢ƒéªŒè¯ (generate.py:327-332)
                assert not (model_config.get('t5_fsdp', False) or model_config.get('dit_fsdp', False)), \
                    "t5_fsdp and dit_fsdp are not supported in non-distributed environments."
                assert not use_usp, \
                    "context parallel are not supported in non-distributed environments."

            max_retries = 3
            for attempt in range(max_retries):
                try:
                    logger.info(f"Rank {rank}: Loading {self.device_type} model attempt {attempt + 1}/{max_retries}")

                    # æ„å»º WanI2V å‚æ•° - ä¸¥æ ¼æŒ‰ç…§ generate.py:421-425 çš„æ–¹å¼
                    wan_config = {
                        'config': cfg,
                        'checkpoint_dir': self.ckpt_dir,
                        'device_id': getattr(self, 'local_rank', 0),
                        'rank': rank,
                        't5_fsdp': model_config.get('t5_fsdp', False),
                        'dit_fsdp': model_config.get('dit_fsdp', True),
                        'use_usp': use_usp,  # è¿™ä¸ªæ˜¯å…³é”®å‚æ•°
                        't5_cpu': model_config.get('t5_cpu', True),
                    }

                    logger.info(f"Creating WanI2V with config: {list(wan_config.keys())}")
                    logger.info(f"Final distributed config: use_usp={use_usp}")

                    # åˆ›å»º WanI2V æ¨¡å‹
                    model = wan.WanI2V(**wan_config)

                    # è®¾å¤‡ç‰¹å®šçš„æ¨¡å‹é…ç½®
                    self._configure_model(model)

                    logger.info(f"Rank {rank}: {self.device_type} model loaded successfully")
                    return model

                except Exception as e:
                    logger.warning(f"Rank {rank}: Model loading attempt {attempt + 1} failed: {str(e)}")
                    if attempt == max_retries - 1:
                        raise
                    
                    time.sleep(5 * (attempt + 1))

        except ImportError as e:
            logger.error(f"âŒ Failed to import wan module: {e}")
            raise
        except Exception as e:
            logger.error(f"Failed to load {self.device_type} model: {str(e)}")
            raise

    def _warmup_t5_cpu(self, model):
        """T5 CPUæ¨¡å¼é¢„çƒ­"""
        if not self.t5_cpu:
            return
        
        try:
            logger.info(f"Rank {self.rank}: Starting T5 CPU warmup...")
            
            # æ‰§è¡Œé¢„çƒ­æ¨ç†
            dummy_text = "warmup text for t5 encoder"
            with torch.no_grad():
                _ = model.text_encoder.encode([dummy_text])
            
            # åˆ†å¸ƒå¼åŒæ­¥
            if self.world_size > 1:
                logger.info(f"Rank {self.rank}: T5 warmup sync...")
                dist.barrier()
            
            logger.info(f"Rank {self.rank}: T5 warmup completed")
            
        except Exception as e:
            logger.warning(f"Rank {self.rank}: T5 warmup failed: {str(e)}")
            if self.world_size > 1:
                try:
                    dist.barrier()
                except Exception as sync_e:
                    logger.error(f"Rank {self.rank}: Failed to sync after warmup failure: {str(sync_e)}")

class VideoGenerationMixin:
    """è§†é¢‘ç”ŸæˆåŠŸèƒ½æ··å…¥ç±»"""
    
    def _generate_video_common(self, request, image_path: str, output_path: str) -> str:
        """é€šç”¨è§†é¢‘ç”Ÿæˆé€»è¾‘"""
        try:
            self._log_memory_usage()
            
            # å¤„ç†å‚æ•°
            size = self._normalize_size(request.image_size or "1280*720")
            img = Image.open(image_path).convert("RGB")
            frame_num = self._validate_frame_num(request.num_frames or 81)
            
            logger.info(f"Generating video with size={size}, frames={frame_num}")
            logger.info(f"{self.device_type} T5 CPU mode: {self.t5_cpu}")
            
            start_time = time.time()
            
            # åˆ†å¸ƒå¼åŒæ­¥
            if self.world_size > 1:
                logger.info(f"Rank {self.rank} waiting at {self.device_type} generation sync barrier")
                dist.barrier()
                logger.info(f"Rank {self.rank} passed {self.device_type} generation sync barrier")
            
            # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šä¼ é€’æ­£ç¡®çš„å‚æ•°ç»™è®¾å¤‡ç‰¹å®šçš„ç”Ÿæˆæ–¹æ³•
            video_tensor = self._generate_video_device_specific(
                request, 
                img, 
                size, 
                frame_num
            )
            
            # ä¿å­˜è§†é¢‘
            if video_tensor is not None:
                logger.info(f"Generated video tensor shape: {video_tensor.shape}")
                self._save_video(video_tensor, output_path, frame_num)
                logger.info(f"Video saved to {output_path}")
            else:
                logger.info(f"Non-master rank {self.rank}, video not saved")
            
            # æœ€ç»ˆåŒæ­¥
            if self.world_size > 1:
                logger.info(f"Rank {self.rank} final {self.device_type} barrier")
                dist.barrier()
            
            generation_time = time.time() - start_time
            logger.info(f"Rank {self.rank}: {self.device_type} generation completed in {generation_time:.2f}s")
            
            self._log_memory_usage()
            return output_path
            
        except Exception as e:
            logger.error(f"{self.device_type} video generation failed: {str(e)}")
            self._empty_cache()
            raise
        
    def _save_video(self, video_tensor, output_path: str, frame_num: int):
        """ä¿å­˜è§†é¢‘"""
        try:
            from wan.utils.utils import cache_video
            cache_video(
                tensor=video_tensor[None],
                save_file=output_path,
                fps=max(8, frame_num // 10),
                nrow=1,
                normalize=True,
                value_range=(-1, 1)
            )
        except Exception as e:
            logger.error(f"Failed to save video: {e}")
            raise
    
    def _validate_frame_num(self, frame_num: int) -> int:
        """éªŒè¯å’Œè°ƒæ•´å¸§æ•°"""
        valid_frames = [41, 61, 81, 121]
        if frame_num in valid_frames:
            return frame_num
        
        closest_frame = min(valid_frames, key=lambda x: abs(x - frame_num))
        logger.warning(f"Frame number {frame_num} adjusted to {closest_frame}")
        return closest_frame
    
    def _normalize_size(self, size: str) -> str:
        """æ ‡å‡†åŒ–å°ºå¯¸å‚æ•°"""
        if size == "auto":
            return "1280*720"
        
        try:
            if '*' in size:
                width, height = map(int, size.split('*'))
                if width > 0 and height > 0 and width <= 4096 and height <= 4096:
                    return size
        except:
            pass
        
        logger.warning(f"Invalid size {size}, using default 1280*720")
        return "1280*720"

class BasePipeline(DistributedMixin, ModelLoaderMixin, VideoGenerationMixin):
    """é‡æ„åçš„åŸºç¡€ç®¡é“ç±» - ä½¿ç”¨æ¨¡æ¿æ–¹æ³•æ¨¡å¼"""
    
    def __init__(self, ckpt_dir: str, **model_args):
        self.ckpt_dir = ckpt_dir
        self.model_args = model_args
        self.model = None
        self.device_type = "unknown"  # å­ç±»éœ€è¦è®¾ç½®
        
        # åˆ†å¸ƒå¼ç¯å¢ƒ
        self.rank = int(os.environ.get("RANK", 0))
        self.world_size = int(os.environ.get("WORLD_SIZE", 1))
        self.local_rank = int(os.environ.get("LOCAL_RANK", 0))
        
        # T5 CPUé…ç½®
        self.t5_cpu = model_args.get('t5_cpu', False)
        
        logger.info(f"Initializing {self.__class__.__name__} on rank {self.rank}")
    
    # æ¨¡æ¿æ–¹æ³• - å®šä¹‰ç®—æ³•éª¨æ¶
    def initialize(self):
        """åˆå§‹åŒ–ç®¡é“ - æ¨¡æ¿æ–¹æ³•"""
        self._setup_environment()
        self._init_distributed()
        self.model = self._load_model()
        logger.info(f"{self.device_type} Pipeline initialized successfully")
    
    def generate_video(self, request, task_id: str, progress_callback: Optional[Callable] = None) -> str:
        """ç”Ÿæˆè§†é¢‘ - æ¨¡æ¿æ–¹æ³•"""
        logger.info(f"Starting video generation for task {task_id} on rank {self.rank}")
        
        try:
            # ä¸‹è½½å›¾ç‰‡ï¼ˆåªåœ¨ rank 0 æ‰§è¡Œï¼‰
            image_path = None
            if self.rank == 0:
                image_path = self._download_image_sync(request.image_url, task_id)
            
            # å¹¿æ’­å›¾ç‰‡è·¯å¾„
            image_path = self._broadcast_image_path(image_path)
            
            # ç”Ÿæˆè§†é¢‘
            output_path = self._get_output_path(task_id)
            result_path = self._generate_video_common(request, image_path, output_path)
            
            logger.info(f"Video generation completed for task {task_id}: {result_path}")
            return result_path
            
        except Exception as e:
            logger.error(f"Video generation failed for task {task_id}: {str(e)}")
            self._empty_cache()
            raise
    
    def cleanup(self):
        """æ¸…ç†èµ„æº - æ¨¡æ¿æ–¹æ³•"""
        try:
            logger.info(f"Cleaning up {self.device_type} pipeline on rank {self.rank}")
            
            if hasattr(self, 'model') and self.model is not None:
                del self.model
                self.model = None
            
            self._empty_cache()
            
            if self.world_size > 1 and dist.is_initialized():
                dist.destroy_process_group()
            
            logger.info(f"{self.device_type} pipeline cleaned up successfully")
            
        except Exception as e:
            logger.error(f"Error during {self.device_type} cleanup: {str(e)}")
    
    # é’©å­æ–¹æ³• - å­ç±»å¯ä»¥é‡å†™è¿™äº›æ–¹æ³•æ¥æä¾›ç‰¹å®šå®ç°
    @abc.abstractmethod
    def _setup_environment(self):
        """è®¾ç½®ç¯å¢ƒå˜é‡ - å­ç±»å®ç°"""
        pass
    
    @abc.abstractmethod
    def _set_device(self):
        """è®¾ç½®è®¾å¤‡ - å­ç±»å®ç°"""
        pass
    
    @abc.abstractmethod
    def _move_to_device(self, tensor):
        """ç§»åŠ¨å¼ é‡åˆ°è®¾å¤‡ - å­ç±»å®ç°"""
        pass
    
    @abc.abstractmethod
    def _configure_model(self, model):
        """é…ç½®æ¨¡å‹ - å­ç±»å®ç°"""
        pass
    
    @abc.abstractmethod
    def _generate_video_device_specific(self, request, img, size: str, frame_num: int):
        """è®¾å¤‡ç‰¹å®šçš„è§†é¢‘ç”Ÿæˆ - å­ç±»å®ç°"""
        pass
    
    @abc.abstractmethod
    def _log_memory_usage(self):
        """è®°å½•å†…å­˜ä½¿ç”¨ - å­ç±»å®ç°"""
        pass
    
    @abc.abstractmethod
    def _empty_cache(self):
        """æ¸…ç†ç¼“å­˜ - å­ç±»å®ç°"""
        pass
    
    # æä¾›é»˜è®¤å®ç°çš„æ–¹æ³•
    def _init_distributed(self):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼ - å­ç±»å¯é‡å†™"""
        backend = self._get_backend()
        timeout_seconds = 2400 if self.t5_cpu else 1800
        self._init_distributed_common(backend, timeout_seconds)
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹ - å­ç±»å¯é‡å†™"""
        model_config = {
            't5_fsdp': self.model_args.get("t5_fsdp", False),
            'dit_fsdp': self.model_args.get("dit_fsdp", False),
            'use_usp': (self.model_args.get("ulysses_size", 1) > 1),
            'ulysses_size': self.model_args.get("ulysses_size", 1),
            'vae_parallel': self.model_args.get("vae_parallel", False),
            'task': self.model_args.get("task", "i2v-14B")
        }
        return self._load_model_common(model_config)
    
    def _broadcast_image_path(self, image_path: Optional[str]) -> str:
        """å¹¿æ’­å›¾ç‰‡è·¯å¾„ - å­ç±»å¯é‡å†™"""
        return self._broadcast_image_path_common(image_path)
    
    def _download_image_sync(self, image_url: str, task_id: str) -> str:
        """åŒæ­¥ä¸‹è½½å›¾ç‰‡"""
        output_dir = Path("generated_videos")
        output_dir.mkdir(exist_ok=True)
        image_path = output_dir / f"{task_id}_input.jpg"
        
        try:
            response = requests.get(image_url, timeout=60)
            response.raise_for_status()
            
            image = Image.open(BytesIO(response.content)).convert("RGB")
            
            # è°ƒæ•´å¤§å°
            max_size = (2048, 2048)
            if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
                image.thumbnail(max_size, Image.Resampling.LANCZOS)
            
            image.save(image_path, quality=95, optimize=True)
            logger.info(f"Image downloaded and saved: {image_path}")
            return str(image_path)
            
        except Exception as e:
            logger.error(f"Failed to download image for task {task_id}: {str(e)}")
            raise
    
    def _get_output_path(self, task_id: str) -> str:
        """è·å–è¾“å‡ºè§†é¢‘è·¯å¾„"""
        output_dir = Path("generated_videos")
        output_dir.mkdir(exist_ok=True)
        return str(output_dir / f"{task_id}.mp4")
    
    @abc.abstractmethod
    def _get_backend(self) -> str:
        """è·å–åˆ†å¸ƒå¼åç«¯ - å­ç±»å®ç°"""
        pass